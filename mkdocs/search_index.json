{
    "docs": [
        {
            "location": "/", 
            "text": "Switch to BEAST mode\n\n  Rampage through code/test iterations for Alexa and Lambda development\n\n\n\n\n\n    \n\n        \n\n    \n\n    \n\n      \n\n    \n\n    \n\n        \n\n    \n\n    \n\n        \n\n    \n\n    \n\n        \n\n    \n\n    \n\n        \n\n    \n\n\n\n\n\n\n\nOverview\n\n\nWith Bespoken, develop faster and better. Do not slow-down for:\n\n\n\n\nTime-consuming server deployments\n\n\nOver-complicated and highly manual testing routines\n\n\nComplex ffmpeg encoding and configuration\n\n\n\n\nThe command-line tools include:\n\n\n\n\nDeploy\n - One-command deployment of Lambda functions  \n\n\nProxy\n - Easily debug Alexa requests by having them send directly to your laptop  \n\n\nSpeak\n and \nIntend\n - Send intents and utterances to your Alexa skill\n\n\nLaunch\n - Send a Launch Request to your Alexa skill\n\n\n\n\nThe library provides:\n\n\n\n\nBSTAlexa\n - Alexa emulator for unit-testing and functional-testing of your skills\n\n\nBSTEncode\n - Encodes audio files for use in SSML without pesky ffmpeg\n\n\nLogless\n - Completely painless, serverless logging\n\n\n\n\nInstallation\n\n\nFor use of the CLI:\n\n\n$ npm install bespoken-tools -g\n\n\n\n\nFor use of the Bespoken library (including Logless and the Emulator):\n\n\n$ npm install bespoken-tools --save\n\n\n\n\nFor additional help, see \nGetting Started\n\n\nTutorials For Alexa Skills\n\n\n\n\nNodejs Lambda Tutorial\n\n\nJava Server Tutorial\n\n\nPython \n Flask-Ask\n\n\nAlexa Emulator Tutorial - Node.js\n\n\n\n\nTutorials For Actions on Google\n\n\n\n\nWith Cloud Functions\n\n\nWith API.AI\n\n\n\n\nTutorials For General Lambdas\n\n\n\n\nRunning Lambdas Locally\n\n\nDebugging Lambdas Locally\n\n\nDeploying Lambdas\n\n\n\n\nQuestions/Feedback?\n\n\nTalk to us on \nGitter\n, also feel free to open an \nissue\n for a bug or feature request.\n\n\nWe love to hear feedback.", 
            "title": "Home"
        }, 
        {
            "location": "/#overview", 
            "text": "With Bespoken, develop faster and better. Do not slow-down for:   Time-consuming server deployments  Over-complicated and highly manual testing routines  Complex ffmpeg encoding and configuration   The command-line tools include:   Deploy  - One-command deployment of Lambda functions    Proxy  - Easily debug Alexa requests by having them send directly to your laptop    Speak  and  Intend  - Send intents and utterances to your Alexa skill  Launch  - Send a Launch Request to your Alexa skill   The library provides:   BSTAlexa  - Alexa emulator for unit-testing and functional-testing of your skills  BSTEncode  - Encodes audio files for use in SSML without pesky ffmpeg  Logless  - Completely painless, serverless logging", 
            "title": "Overview"
        }, 
        {
            "location": "/#installation", 
            "text": "For use of the CLI:  $ npm install bespoken-tools -g  For use of the Bespoken library (including Logless and the Emulator):  $ npm install bespoken-tools --save  For additional help, see  Getting Started", 
            "title": "Installation"
        }, 
        {
            "location": "/#tutorials-for-alexa-skills", 
            "text": "Nodejs Lambda Tutorial  Java Server Tutorial  Python   Flask-Ask  Alexa Emulator Tutorial - Node.js", 
            "title": "Tutorials For Alexa Skills"
        }, 
        {
            "location": "/#tutorials-for-actions-on-google", 
            "text": "With Cloud Functions  With API.AI", 
            "title": "Tutorials For Actions on Google"
        }, 
        {
            "location": "/#tutorials-for-general-lambdas", 
            "text": "Running Lambdas Locally  Debugging Lambdas Locally  Deploying Lambdas", 
            "title": "Tutorials For General Lambdas"
        }, 
        {
            "location": "/#questionsfeedback", 
            "text": "Talk to us on  Gitter , also feel free to open an  issue  for a bug or feature request.  We love to hear feedback.", 
            "title": "Questions/Feedback?"
        }, 
        {
            "location": "/getting_started/", 
            "text": "Installation\n\n\nMake sure you have NPM and node installed:\n\n\n$ node --version \n npm --version\n\n\n\n\nWe support node version \n4.x.x\n and above.  For help installing, see \nHow To Install NPM\n\n\nNext, install the Bespoken command line tool (bst):\n\n\n$ npm install bespoken-tools -g\n\n\n\n\nNote:\n If you are on MacOS and the command fails, it is probably because you need to run it with sudo, like this:\n\n\n$ sudo npm install bespoken-tools -g\n\n\n\n\nVerify the installation by typing:\n\n\n$ bst\n\n\n\n\nTo work with the Bespoken API, install it with your project as part of the package.json:\n\n\nnpm install bespoken-tools --save\n\n\n\n\nYou will then be able to use our:\n\n\n\n\nBSTAlexa\n emulator \n\n\nBSTEncode\n audio encoder\n\n\nLogless\n effortless logging and diagnostics service\n\n\n\n\nUpdating\n\n\nTo update bst:\n\n\n$ npm update bespoken-tools -g", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting_started/#installation", 
            "text": "Make sure you have NPM and node installed:  $ node --version   npm --version  We support node version  4.x.x  and above.  For help installing, see  How To Install NPM  Next, install the Bespoken command line tool (bst):  $ npm install bespoken-tools -g  Note:  If you are on MacOS and the command fails, it is probably because you need to run it with sudo, like this:  $ sudo npm install bespoken-tools -g  Verify the installation by typing:  $ bst  To work with the Bespoken API, install it with your project as part of the package.json:  npm install bespoken-tools --save  You will then be able to use our:   BSTAlexa  emulator   BSTEncode  audio encoder  Logless  effortless logging and diagnostics service", 
            "title": "Installation"
        }, 
        {
            "location": "/getting_started/#updating", 
            "text": "To update bst:  $ npm update bespoken-tools -g", 
            "title": "Updating"
        }, 
        {
            "location": "/alexa_skills_kit_configuration/", 
            "text": "In order to leverage the bst proxy functionality, you must configure your skill from the Amazon Developer Console to point to the bst proxy server.\n\n\n\n\nConfigure your Skill Endpoint\n\n\nWhen you run the bst proxy command, you will see a URL output:\n\n\n$ bst proxy lambda index.js\nBST: v1.0.4  Node: v4.3.2\n\nYour public URL for accessing your local service:\nhttps://determined-rice.bespoken.link\n\n\n\n\nFrom your \nSkill's list\n, select an existing skill to edit or create a new one.  Navigate to the Configuration step and update the HTTPS endpoint to the one provided by the proxy command.\n\n\n\n\nMake sure Account linking is set to \"No\" and on the SSL Certificate step select \"My development endpoint is a subdomain of a domain that has a wildcard certificate from a certificate authority\".\n\n\n Note\n To help generate your configuration URL, you can also use \nbst proxy urlgen\n\n\nPath Component for proxy http\n\n\nWhen using bst proxy http for local servers running on a specific port, you need to make sure your path components are correct.\n\n\nFor example, when you run the bst proxy http command:\n\n\n$ bst proxy http 9999\nBST: v1.0.4  Node: v4.3.2\n\nYour public URL for accessing your local service:\nhttps://determined-rice.bespoken.link\n\n\n\n\nIf your local server listens on path \n/alexa-skill\n, you need to update the URL with your path before configuring your Skill.\n\n\nYour public URL for accessing your local service:\nhttps://determined-rice.bespoken.link/alexa-skill", 
            "title": "Alexa Skill Configuration"
        }, 
        {
            "location": "/alexa_skills_kit_configuration/#configure-your-skill-endpoint", 
            "text": "When you run the bst proxy command, you will see a URL output:  $ bst proxy lambda index.js\nBST: v1.0.4  Node: v4.3.2\n\nYour public URL for accessing your local service:\nhttps://determined-rice.bespoken.link  From your  Skill's list , select an existing skill to edit or create a new one.  Navigate to the Configuration step and update the HTTPS endpoint to the one provided by the proxy command.   Make sure Account linking is set to \"No\" and on the SSL Certificate step select \"My development endpoint is a subdomain of a domain that has a wildcard certificate from a certificate authority\".   Note  To help generate your configuration URL, you can also use  bst proxy urlgen", 
            "title": "Configure your Skill Endpoint"
        }, 
        {
            "location": "/alexa_skills_kit_configuration/#path-component-for-proxy-http", 
            "text": "When using bst proxy http for local servers running on a specific port, you need to make sure your path components are correct.  For example, when you run the bst proxy http command:  $ bst proxy http 9999\nBST: v1.0.4  Node: v4.3.2\n\nYour public URL for accessing your local service:\nhttps://determined-rice.bespoken.link  If your local server listens on path  /alexa-skill , you need to update the URL with your path before configuring your Skill.  Your public URL for accessing your local service:\nhttps://determined-rice.bespoken.link/alexa-skill", 
            "title": "Path Component for proxy http"
        }, 
        {
            "location": "/commands/deploy/", 
            "text": "Overview\n\n\nThe deploy command allows you to install and update a Node/JavaScript lambda skill in the AWS cloud. \nYou can make changes to your code and quickly install it to see if it works on your device (like an Echo).\n\n\nThe command is described below.\n\n\nbst deploy lambda\n\n\nOverview\n  \n\n\nThe deploy lambda command allows you to deploy a Node/JavaScript lambda function to AWS\n\n\nNotes\n\n\n\n\nThe command currently only supports Node/JavaScript Lambdas and only works on OS X and Linux\n\n\nYou need to have an AWS account with appropriate privileges and credentials\n\n\nConsidering the options and complexity of the Amazon web services, this tool wasn't meant to be a production deploy tool,\n  but it will help you to quickly verify your skill on the device\n\n\nnpm\n and \nzip\n commands need to be installed and seen in your $PATH. On OS X the zip is installed by default. \n  On Linux you may have to install it, depending on your distribution. \n\n\n\n\nUsage\n  \n\n\nTo use it, invoke it with a Node/JavaScript Lambda project folder to install.\n\n\nSyntax:\n\n\n$ bst deploy lambda \nPATH_TO_LAMBDA_PROJECT\n\n\n\n\n\nExample:  \n\n\n$ bst deploy lambda ../test/resources/deployProject\n\n\n\n\nYou can learn more here at our \nNode.js Tutorial\n:\n\n\nOptions\n\n\nThe \n--verbose\n flag will print out a more detailed chit-chat about what's happening.\n\n\nTo use it, just start up the command with\n\n\n$ bst deploy lambda ../test/resources/deployProject --verbose\n\n\n\n\nWith the \n--lambdaName\n option you can name your lambda function. By default the tool will use your folder name.\n\n\nTo use it, just start up the command with\n\n\n$ bst deploy lambda ../test/resources/deployProject --lambdaName BestSkillEver\n\n\n\n\nThe function will be named BestSkillEver instead of the deployProject. \n\n\nConfiguration\n\n\nInstalling a lambda requires the creation of a few extras, like an IAM role with policy. Also you need to specify a few other things.\nThis tool is a quick test tool, it will install the function with some reasonable parameters. You can tweak a them\nin the BST configuration file (.bst/config in your home folder). BST tools will create the config file for you at the first run. \nIt is a JSON file. It looks like this below. This command uses the \nlambdaDeploy\n section of the file.\n\n\n{\n    \nnodeID\n: \nfdd376c8-3d3e-4a48-8b74-4d80a11924af\n,\n    \nlambdaDeploy\n: {\n        \nruntime\n: \nnodejs4.3\n,\n        \nrole\n: \nlambda-bst-execution\n,\n        \nhandler\n: \nindex.handler\n,\n        \ndescription\n: \nMy BST lambda skill\n,\n        \ntimeout\n: 3,\n        \nmemorySize\n: 128,\n        \nvpcSubnets\n: \n,\n        \nvpcSecurityGroups\n: \n,\n        \nexcludeGlobs\n: \nevent.json\n\n    }\n}\n\n\n\n\nAgain, this tool is to get you going quickly with your lambda function, so the configuration parameters are far from the available\ntweaks AWS offers. The fields are self explanatory (I hope).\n\n\nNotes about AWS\n\n\n\n\n\n\nThe command will try to use your credentials from the usual AWS config folder (.aws/credentials in your home folder). \nIf you installed the aws command line tools you probably already have it.\nAlternatively you can set the \nAWS_SECRET_ACCESS_KEY\n and \nAWS_ACCESS_KEY_ID\n shell environment variables to provide the keys.\n\n\n\n\n\n\nThe command will create automatically a basic role for your Lambda function (a role your lambda assumes when it runs). \nThis role has access to Dynamo for persistence, AWS logs and S3. If you need more, add more policies or use your own role in BST config above.\n\n\n\n\n\n\nNotes about the Lambda project folder\n\n\n\n\nThe tool uses \nnpm\n and \nzip\n under the hood to package your project. Make sure your project follows the conventions \nof the Node.js projects. With --verbose you can see the actual zip file path that was deployed to AWS. \nTake a peek inside to make sure you have all the pieces (dependencies for instance) if your lambda function doesn't work.", 
            "title": "bst deploy"
        }, 
        {
            "location": "/commands/deploy/#overview", 
            "text": "The deploy command allows you to install and update a Node/JavaScript lambda skill in the AWS cloud. \nYou can make changes to your code and quickly install it to see if it works on your device (like an Echo).  The command is described below.", 
            "title": "Overview"
        }, 
        {
            "location": "/commands/deploy/#bst-deploy-lambda", 
            "text": "Overview     The deploy lambda command allows you to deploy a Node/JavaScript lambda function to AWS  Notes   The command currently only supports Node/JavaScript Lambdas and only works on OS X and Linux  You need to have an AWS account with appropriate privileges and credentials  Considering the options and complexity of the Amazon web services, this tool wasn't meant to be a production deploy tool,\n  but it will help you to quickly verify your skill on the device  npm  and  zip  commands need to be installed and seen in your $PATH. On OS X the zip is installed by default. \n  On Linux you may have to install it, depending on your distribution.    Usage     To use it, invoke it with a Node/JavaScript Lambda project folder to install.  Syntax:  $ bst deploy lambda  PATH_TO_LAMBDA_PROJECT   Example:    $ bst deploy lambda ../test/resources/deployProject  You can learn more here at our  Node.js Tutorial :  Options  The  --verbose  flag will print out a more detailed chit-chat about what's happening.  To use it, just start up the command with  $ bst deploy lambda ../test/resources/deployProject --verbose  With the  --lambdaName  option you can name your lambda function. By default the tool will use your folder name.  To use it, just start up the command with  $ bst deploy lambda ../test/resources/deployProject --lambdaName BestSkillEver  The function will be named BestSkillEver instead of the deployProject.   Configuration  Installing a lambda requires the creation of a few extras, like an IAM role with policy. Also you need to specify a few other things.\nThis tool is a quick test tool, it will install the function with some reasonable parameters. You can tweak a them\nin the BST configuration file (.bst/config in your home folder). BST tools will create the config file for you at the first run. \nIt is a JSON file. It looks like this below. This command uses the  lambdaDeploy  section of the file.  {\n     nodeID :  fdd376c8-3d3e-4a48-8b74-4d80a11924af ,\n     lambdaDeploy : {\n         runtime :  nodejs4.3 ,\n         role :  lambda-bst-execution ,\n         handler :  index.handler ,\n         description :  My BST lambda skill ,\n         timeout : 3,\n         memorySize : 128,\n         vpcSubnets :  ,\n         vpcSecurityGroups :  ,\n         excludeGlobs :  event.json \n    }\n}  Again, this tool is to get you going quickly with your lambda function, so the configuration parameters are far from the available\ntweaks AWS offers. The fields are self explanatory (I hope).  Notes about AWS    The command will try to use your credentials from the usual AWS config folder (.aws/credentials in your home folder). \nIf you installed the aws command line tools you probably already have it.\nAlternatively you can set the  AWS_SECRET_ACCESS_KEY  and  AWS_ACCESS_KEY_ID  shell environment variables to provide the keys.    The command will create automatically a basic role for your Lambda function (a role your lambda assumes when it runs). \nThis role has access to Dynamo for persistence, AWS logs and S3. If you need more, add more policies or use your own role in BST config above.    Notes about the Lambda project folder   The tool uses  npm  and  zip  under the hood to package your project. Make sure your project follows the conventions \nof the Node.js projects. With --verbose you can see the actual zip file path that was deployed to AWS. \nTake a peek inside to make sure you have all the pieces (dependencies for instance) if your lambda function doesn't work.", 
            "title": "bst deploy lambda"
        }, 
        {
            "location": "/commands/intend/", 
            "text": "Overview\n\n\nThe intend command generates intent requests for your service as if they were coming from Alexa itself.\n\n\nIt works in a manner very similar to the Alexa simulator available via the Alexa developer console.  \n\n\nTo start using it, you will need a local file that contains your Intent Schema and Sample Utterances.\n\nBy default, we have adopted the pattern used by the Alexa Skills Sample projects (see \nhere\n).\n\n\nThat is, we look for the Interaction Model files inside a folder called speechAssets located off the source root.  \n\n\nYou can specify an alternative location via options to the command-line.\n\n\nIntending\n\n\nTo invoke the intend command, simply type:\n\n\n$ bst intend \nINTENT_NAME\n [SlotName=SlotValue...]\n\n\n\n\nFor example:\n\n\n$ bst intend HelloIntent SlotA=SlotValue\n\n\n\n\nThe intend command will return the full request and response of the interaction with your Skill service.  \n\n\nBy default, the system will:\n\n\n\n\nUse the Intent Model and Sample Utterances in the speechAssets folder under the current working directory\n\n\nUse the service currently running via the \nbst proxy\n command\n\n\n\n\nIf no service is currently running via bst proxy, and HTTP endpoint can be specified with the \n--url\n option:\n\n\n$ bst intend HelloIntent --url https://my.skill.com/skill/path\n\n\n\n\nSpeech Asset Format and Location\n\n\nIf your speech assets  (Intent Model and Sample Utterances) are not stored under ./speechAssets, you can use an option to specify another location.\n\n\nBy default, we look for:\n\n\n\n\n./speechAssets/IntentSchema.json\n\n\n./speechAssets/SampleUtterances.txt\n\n\n\n\nExample:\n\n\n$ bst intend HelloIntent -i interactions/IntentSchema.json -s interactions/SampleUtterances.txt\n\n\n\n\nThe format of these files is the same as they are entered in the Alexa Skill configuration.  \n\n\nThe Intent Schema is a JSON file. Samples utterances is a space-delimited text file.\n\n\nAn example of these files can be found \nhere\n.", 
            "title": "bst intend"
        }, 
        {
            "location": "/commands/intend/#overview", 
            "text": "The intend command generates intent requests for your service as if they were coming from Alexa itself.  It works in a manner very similar to the Alexa simulator available via the Alexa developer console.    To start using it, you will need a local file that contains your Intent Schema and Sample Utterances. \nBy default, we have adopted the pattern used by the Alexa Skills Sample projects (see  here ).  That is, we look for the Interaction Model files inside a folder called speechAssets located off the source root.    You can specify an alternative location via options to the command-line.", 
            "title": "Overview"
        }, 
        {
            "location": "/commands/intend/#intending", 
            "text": "To invoke the intend command, simply type:  $ bst intend  INTENT_NAME  [SlotName=SlotValue...]  For example:  $ bst intend HelloIntent SlotA=SlotValue  The intend command will return the full request and response of the interaction with your Skill service.    By default, the system will:   Use the Intent Model and Sample Utterances in the speechAssets folder under the current working directory  Use the service currently running via the  bst proxy  command   If no service is currently running via bst proxy, and HTTP endpoint can be specified with the  --url  option:  $ bst intend HelloIntent --url https://my.skill.com/skill/path", 
            "title": "Intending"
        }, 
        {
            "location": "/commands/intend/#speech-asset-format-and-location", 
            "text": "If your speech assets  (Intent Model and Sample Utterances) are not stored under ./speechAssets, you can use an option to specify another location.  By default, we look for:   ./speechAssets/IntentSchema.json  ./speechAssets/SampleUtterances.txt   Example:  $ bst intend HelloIntent -i interactions/IntentSchema.json -s interactions/SampleUtterances.txt  The format of these files is the same as they are entered in the Alexa Skill configuration.    The Intent Schema is a JSON file. Samples utterances is a space-delimited text file.  An example of these files can be found  here .", 
            "title": "Speech Asset Format and Location"
        }, 
        {
            "location": "/commands/proxy/", 
            "text": "Overview\n\n\nThe proxy command allows you to interact with a local service running on your machine via an Alexa device.  Using it, you can make changes to code running on your machine and immediately make them available via an Echo or the Alexa simulator.\n\n\nThe proxy tool works either directly with Node/JavaScript lambda code - \nproxy lambda\n.  Or it can proxy any http service using \nproxy http\n.  \n\n\nThe two commands are described below, as well as the urlgen helper command.\n\n\nbst proxy lambda\n\n\nOverview\n  \n\n\nThe proxy lambda command allows you to run a Lambda as a local service your machine.\n\n\nNote\n\n\n\n\nThe command currently only supports Node/JavaScript Lambdas.\n\n\n\n\nUsage\n  \n\n\nTo use it, invoke it with the Lambda file to run.  The proxy will automatically use the latest code in your working directory.\n\n\nSyntax:\n\n\n$ bst proxy lambda \nPATH_TO_LAMBDA\n\n\n\n\n\nExample:  \n\n\n$ bst proxy lambda index.js\n\n\n\n\nYou can learn more here at our \nNode.js Tutorial\n:\n\n\nOptions\n\n\nThe \n--verbose\n flag will print out the requests and responses from all calls made to your skill.\n\n\nTo use it, just start up the proxy with:\n\n\n$ bst proxy lambda index.js --verbose\n\n\n\n\nbst proxy http\n\n\nOverview\n  \n\n\nProxy http allows you to interact with a local service running on your machine (on a port) via an Alexa device.\n\n\nUsage\n  \n\n\nThe proxy http command takes one command, the \n that your local Alexa service is listening on.  All traffic coming from Alexa will be forwarded to it.\n\n\nSyntax:\n\n\n$ bst proxy http \nPORT\n\n\n\n\n\nExample:\n\n\n$ bst proxy http 9999\n\n\n\n\nYou can learn more here at our \nJava Tutorial\n\n\nOptions\n\n\nThe \n--verbose\n flag will print out the requests and responses from all calls made to your skill.\n\n\nTo use it, just start up the proxy with:\n\n\n$ bst proxy http 9999 --verbose\n\n\n\n\nbst proxy urlgen\n\n\nOverview\n\n\nYour skill must be setup to point at our server. For example, if the URL for your skill is normally:\n\n\nhttps://myskill.example.com/skillA\n\n\n\n\nIt should instead be configured to point at the bst proxy server, like so:\n\n\nhttps://proxy.bespoken.tools/skillA?node-id=1b84270f-5b58-4176-a8b6-7b5b1c03a308\n\n\n\n\nNote the Node ID passed in the query string.  This is a UUID that ties off your local proxy with our server.\n\n\nThe rest of the URL path and query string should be unchanged.\n\n\nFor more information on configuring your Skill see \nConfiguring your Skill for bst proxy\n.\n\n\nUsage\n\n\nThe proxy urlgen command can help generate the endpoint.\n\n\nSyntax:\n\n\n$ bst proxy urlgen \nURL\n\n\n\n\n\nExample:\n\n\n$ bst proxy urlgen https://myskill.example.com/skillA\n\n\n\n\nThe above example command will then provide you with HTTPS endpoint that is required during the configuration step when you setup your Alexa Skill.", 
            "title": "bst proxy"
        }, 
        {
            "location": "/commands/proxy/#overview", 
            "text": "The proxy command allows you to interact with a local service running on your machine via an Alexa device.  Using it, you can make changes to code running on your machine and immediately make them available via an Echo or the Alexa simulator.  The proxy tool works either directly with Node/JavaScript lambda code -  proxy lambda .  Or it can proxy any http service using  proxy http .    The two commands are described below, as well as the urlgen helper command.", 
            "title": "Overview"
        }, 
        {
            "location": "/commands/proxy/#bst-proxy-lambda", 
            "text": "Overview     The proxy lambda command allows you to run a Lambda as a local service your machine.  Note   The command currently only supports Node/JavaScript Lambdas.   Usage     To use it, invoke it with the Lambda file to run.  The proxy will automatically use the latest code in your working directory.  Syntax:  $ bst proxy lambda  PATH_TO_LAMBDA   Example:    $ bst proxy lambda index.js  You can learn more here at our  Node.js Tutorial :  Options  The  --verbose  flag will print out the requests and responses from all calls made to your skill.  To use it, just start up the proxy with:  $ bst proxy lambda index.js --verbose", 
            "title": "bst proxy lambda"
        }, 
        {
            "location": "/commands/proxy/#bst-proxy-http", 
            "text": "Overview     Proxy http allows you to interact with a local service running on your machine (on a port) via an Alexa device.  Usage     The proxy http command takes one command, the   that your local Alexa service is listening on.  All traffic coming from Alexa will be forwarded to it.  Syntax:  $ bst proxy http  PORT   Example:  $ bst proxy http 9999  You can learn more here at our  Java Tutorial  Options  The  --verbose  flag will print out the requests and responses from all calls made to your skill.  To use it, just start up the proxy with:  $ bst proxy http 9999 --verbose", 
            "title": "bst proxy http"
        }, 
        {
            "location": "/commands/proxy/#bst-proxy-urlgen", 
            "text": "Overview  Your skill must be setup to point at our server. For example, if the URL for your skill is normally:  https://myskill.example.com/skillA  It should instead be configured to point at the bst proxy server, like so:  https://proxy.bespoken.tools/skillA?node-id=1b84270f-5b58-4176-a8b6-7b5b1c03a308  Note the Node ID passed in the query string.  This is a UUID that ties off your local proxy with our server.  The rest of the URL path and query string should be unchanged.  For more information on configuring your Skill see  Configuring your Skill for bst proxy .  Usage  The proxy urlgen command can help generate the endpoint.  Syntax:  $ bst proxy urlgen  URL   Example:  $ bst proxy urlgen https://myskill.example.com/skillA  The above example command will then provide you with HTTPS endpoint that is required during the configuration step when you setup your Alexa Skill.", 
            "title": "bst proxy urlgen"
        }, 
        {
            "location": "/commands/speak/", 
            "text": "Overview\n\n\nThe speak command generates intent requests for your service as if they were coming from Alexa itself.\n\n\nIt works in a manner very similar to the Alexa simulator available via the Alexa developer console.  \n\n\nTo start using it, you will need a local file that contains your Intent Schema and Sample Utterances.\n\nBy default, we have adopted the pattern used by the Alexa Skills Sample projects (see \nhere\n).\n\n\nThat is, we look for the Interaction Model files inside a folder called speechAssets located off the source root.  \n\n\nYou can specify an alternative location via options to the command-line.\n\n\nSpeaking\n\n\nTo invoke the speak command, simply type:\n\n\n$ bst speak \nUTTERANCE\n\n\n\n\n\nFor example:\n\n\n$ bst speak Hello World\n\n\n\n\nThe speak command will return the full request and response of the interaction with your Skill service.  \n\n\nBy default, the system will:\n\n\n\n\nUse the Intent Model and Sample Utterances in the speechAssets folder under the current working directory\n\n\nUse the service currently running via the \nbst proxy\n command\n\n\n\n\nIf no service is currently running via bst proxy, and HTTP endpoint can be specified with the \n--url\n option:\n\n\n$ bst speak Hello World --url https://my.skill.com/skill/path\n\n\n\n\nSpeech Asset Format and Location\n\n\nIf your speech assets  (Intent Model and Sample Utterances) are not stored under ./speechAssets, you can use an option to specify another location.\n\n\nBy default, we look for:\n\n\n\n\n./speechAssets/IntentSchema.json\n\n\n./speechAssets/SampleUtterances.txt\n\n\n\n\nExample:\n\n\n$ bst speak Hello World -i interactions/IntentSchema.json -s interactions/SampleUtterances.txt\n\n\n\n\nThe format of these files is the same as they are entered in the Alexa Skill configuration.  \n\n\nThe Intent Schema is a JSON file. Samples utterances is a space-delimited text file.\n\n\nAn example of these files can be found \nhere\n.\n\n\nWorking With Slots\n\n\nSlot handling is a bit tricky. To send an utterance that uses slots, surround the slot variables like so:  \n\n\n{MySlot}\n\n\n\n\nFor example, if the sample utterance was defined as:\n\n\nHelloWorld Hello world, my name is {Name}\n\n\n\n\nThen the speak command would be:\n\n\n$ bst speak Hello World, my name is {John}\n\n\n\n\nThe value \nJohn\n will then be automatically placed in the Name slot for the utterance on the request.\n\n\nOther Notes\n\n\nDefault Utterances\n  \n\n\nWe currently use a simple algorithm to pick out a default Intent if none of the sample utterances match the supplied utterance.  \n\n\nThis algorithm is - the first utterance of the first intent defined in the sample file.  \n\n\nThis is meant to loosely mimic the behavior of the Alexa Simulator, which also seems to randomly select an intent/utterance when none matches.", 
            "title": "bst speak"
        }, 
        {
            "location": "/commands/speak/#overview", 
            "text": "The speak command generates intent requests for your service as if they were coming from Alexa itself.  It works in a manner very similar to the Alexa simulator available via the Alexa developer console.    To start using it, you will need a local file that contains your Intent Schema and Sample Utterances. \nBy default, we have adopted the pattern used by the Alexa Skills Sample projects (see  here ).  That is, we look for the Interaction Model files inside a folder called speechAssets located off the source root.    You can specify an alternative location via options to the command-line.", 
            "title": "Overview"
        }, 
        {
            "location": "/commands/speak/#speaking", 
            "text": "To invoke the speak command, simply type:  $ bst speak  UTTERANCE   For example:  $ bst speak Hello World  The speak command will return the full request and response of the interaction with your Skill service.    By default, the system will:   Use the Intent Model and Sample Utterances in the speechAssets folder under the current working directory  Use the service currently running via the  bst proxy  command   If no service is currently running via bst proxy, and HTTP endpoint can be specified with the  --url  option:  $ bst speak Hello World --url https://my.skill.com/skill/path", 
            "title": "Speaking"
        }, 
        {
            "location": "/commands/speak/#speech-asset-format-and-location", 
            "text": "If your speech assets  (Intent Model and Sample Utterances) are not stored under ./speechAssets, you can use an option to specify another location.  By default, we look for:   ./speechAssets/IntentSchema.json  ./speechAssets/SampleUtterances.txt   Example:  $ bst speak Hello World -i interactions/IntentSchema.json -s interactions/SampleUtterances.txt  The format of these files is the same as they are entered in the Alexa Skill configuration.    The Intent Schema is a JSON file. Samples utterances is a space-delimited text file.  An example of these files can be found  here .", 
            "title": "Speech Asset Format and Location"
        }, 
        {
            "location": "/commands/speak/#working-with-slots", 
            "text": "Slot handling is a bit tricky. To send an utterance that uses slots, surround the slot variables like so:    {MySlot}  For example, if the sample utterance was defined as:  HelloWorld Hello world, my name is {Name}  Then the speak command would be:  $ bst speak Hello World, my name is {John}  The value  John  will then be automatically placed in the Name slot for the utterance on the request.", 
            "title": "Working With Slots"
        }, 
        {
            "location": "/commands/speak/#other-notes", 
            "text": "Default Utterances     We currently use a simple algorithm to pick out a default Intent if none of the sample utterances match the supplied utterance.    This algorithm is - the first utterance of the first intent defined in the sample file.    This is meant to loosely mimic the behavior of the Alexa Simulator, which also seems to randomly select an intent/utterance when none matches.", 
            "title": "Other Notes"
        }, 
        {
            "location": "/commands/launch/", 
            "text": "Overview\n\n\nThe launch command send a launch request to your service as if it was comming from alexa itself.\n\n\nTo start using it, you will need to support the \"LaunchRequest\" event on your handler for the received Intents in your service.\n\n\nUsage\n\n\nTo invoke the launch command, simply type:\n\n\n$ bst launch\n\n\n\n\nThe launch command will return the full request and response of the interaction with your Skill service.\n\n\nBy default, the system will use the service currently running via the \nbst proxy\n command\n\n\nWorking without using the proxy\n\n\nIf no service is currently running via bst proxy, and HTTP endpoint can be specified with the \n--url\n option:\n\n\n$ bst launch --url https://my.skill.com/skill/path", 
            "title": "bst launch"
        }, 
        {
            "location": "/commands/launch/#overview", 
            "text": "The launch command send a launch request to your service as if it was comming from alexa itself.  To start using it, you will need to support the \"LaunchRequest\" event on your handler for the received Intents in your service.", 
            "title": "Overview"
        }, 
        {
            "location": "/commands/launch/#usage", 
            "text": "To invoke the launch command, simply type:  $ bst launch  The launch command will return the full request and response of the interaction with your Skill service.  By default, the system will use the service currently running via the  bst proxy  command", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/launch/#working-without-using-the-proxy", 
            "text": "If no service is currently running via bst proxy, and HTTP endpoint can be specified with the  --url  option:  $ bst launch --url https://my.skill.com/skill/path", 
            "title": "Working without using the proxy"
        }, 
        {
            "location": "/api_nav/", 
            "text": "The Bespoken API reference can be found \nhere\n.", 
            "title": "API Reference"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_nodejs/", 
            "text": "This tutorial shows you how to get started developing for Alexa Skills Kit using a Nodejs Lambda.  \n\n\nPrerequisites\n\n\n\n\nBespoken command-line tools (bst)\n\n\n$ npm install bespoken-tools -g\n\n\nInstallation Instructions\n\n\n\n\n\n\nAmazon Developer Account\n\n\nAmazon Developer\n\n\n\n\n\n\n\n\nGetting Started\n\n\nClone the Amazon Alexa Skills Kit for JavaScript repo:  \n\n\n$ git clone https://github.com/alexa/skill-sample-nodejs-hello-world\n\n\n\n\nGo to the source code directory of the sample:\n\n\n$ cd skill-sample-nodejs-hello-world/src\n\n\n\n\nRun npm install to bring in dependencies:  \n\n\n$ npm install\n\n\n\n\nStart bst proxy\n\n\nFor Nodejs Lambdas, bst proxy command, in addition to setting up the proxy, will run your lambda for you and even reload it on changes.\n\n\nThis will start the helloWorld lambda:\n\n\n$ bst proxy lambda index.js\n\n\n\n\nConfigure your Skill\n\n\nFrom the \nAlexa Skills Kit list\n within the Amazon Developer's Console:\n\n\nChoose \"Add a New Skill\"\n\n\nFill out the Information tab\n\n\n\n\nGive your skill a name and invocation phrase, 'bst nodejs sample' and 'greeter' for example\n\n\n\n\nFill out the Interaction Model\n\n\n\n\nCopy and paste the Intent Schema from \nhere\n\n\nCopy and paste the Sample Utterances from \nhere\n\n\n\n\nConfigure the Endpoint\n\n\nWhen you started the proxy, bst printed out a URL that you need to configure your skill:\n\n\n$ bst proxy lambda samples/helloWorld/src/index.js\nBST: v0.6.5  Node: v4.3.2\n\nYour URL for Alexa Skill configuration:\nhttps://proxy.bespoken.tools?node-id=0c6a4f17-c86f-4024-ba26-a351ac319431\n\n\n\n\nAlternatively, you can create this URL via the \nproxy urlgen\n command.\n\n\nCopy and paste this URL as your endpoint:\n\n\n\n\nAlso make sure you select \"HTTPS\" and account linking to \"NO\".\n\n\nConfigure SSL\n  \n\n\nOn the SSL Certificate page, select the middle radio button \"My development endpoint is a subdomain of a domain that has a wildcard certificate from a certificate authority\"\n\n\nTest\n\n\nGo to the service simulator, and type: \"hello\" and hit \"Ask {Your Skill Name}\".\n\n\nYou should get a valid JSON in reply:\n\n\n\n\nNext Steps\n\n\nYou can now start adding functionality to your skill. To learn more about coding Alexa Skills, see the official \ndocumentation\n\n\nYou can also try it out on an Alexa device like an Echo, as long as it is registered with your account.\nJust say \"Open {Your Invocation Name}\" to use it.", 
            "title": "Node.js Lambda"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_nodejs/#prerequisites", 
            "text": "Bespoken command-line tools (bst)  $ npm install bespoken-tools -g  Installation Instructions    Amazon Developer Account  Amazon Developer", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_nodejs/#getting-started", 
            "text": "Clone the Amazon Alexa Skills Kit for JavaScript repo:    $ git clone https://github.com/alexa/skill-sample-nodejs-hello-world  Go to the source code directory of the sample:  $ cd skill-sample-nodejs-hello-world/src  Run npm install to bring in dependencies:    $ npm install", 
            "title": "Getting Started"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_nodejs/#start-bst-proxy", 
            "text": "For Nodejs Lambdas, bst proxy command, in addition to setting up the proxy, will run your lambda for you and even reload it on changes.  This will start the helloWorld lambda:  $ bst proxy lambda index.js", 
            "title": "Start bst proxy"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_nodejs/#configure-your-skill", 
            "text": "From the  Alexa Skills Kit list  within the Amazon Developer's Console:  Choose \"Add a New Skill\"  Fill out the Information tab   Give your skill a name and invocation phrase, 'bst nodejs sample' and 'greeter' for example   Fill out the Interaction Model   Copy and paste the Intent Schema from  here  Copy and paste the Sample Utterances from  here   Configure the Endpoint  When you started the proxy, bst printed out a URL that you need to configure your skill:  $ bst proxy lambda samples/helloWorld/src/index.js\nBST: v0.6.5  Node: v4.3.2\n\nYour URL for Alexa Skill configuration:\nhttps://proxy.bespoken.tools?node-id=0c6a4f17-c86f-4024-ba26-a351ac319431  Alternatively, you can create this URL via the  proxy urlgen  command.  Copy and paste this URL as your endpoint:   Also make sure you select \"HTTPS\" and account linking to \"NO\".  Configure SSL     On the SSL Certificate page, select the middle radio button \"My development endpoint is a subdomain of a domain that has a wildcard certificate from a certificate authority\"", 
            "title": "Configure your Skill"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_nodejs/#test", 
            "text": "Go to the service simulator, and type: \"hello\" and hit \"Ask {Your Skill Name}\".  You should get a valid JSON in reply:", 
            "title": "Test"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_nodejs/#next-steps", 
            "text": "You can now start adding functionality to your skill. To learn more about coding Alexa Skills, see the official  documentation  You can also try it out on an Alexa device like an Echo, as long as it is registered with your account.\nJust say \"Open {Your Invocation Name}\" to use it.", 
            "title": "Next Steps"
        }, 
        {
            "location": "/tutorials/tutorial_local_server_java/", 
            "text": "This tutorial shows you how to get started developing for Alexa with Java and Maven.\n\n\nPrerequisites\n\n\n\n\nbespoken tools (bst)\n\n\n$ npm install bespoken-tools -g\n\n\nInstallation Instructions\n\n\n\n\n\n\nmaven\n\n\nOSX with homebrew: \n$ brew install maven\n\n\nInstallation Instructions\n\n\n\n\n\n\nAmazon Developer Account\n\n\nAmazon Developer\n\n\n\n\n\n\n\n\nGetting Started\n\n\nClone the bst project:\n\n\n$ git clone https://github.com/bespoken/bst\n\n\n\n\nGo to the sample java skill\n\n\n$ cd bst/samples/java\n\n\n\n\nRun the Sample Java Skill\n\n\nFrom within bst/samples/java directory, compile the example with this command:  \n\n\n$ mvn compile\n\n\n\n\nRun the server with this command:  \n\n\n$ mvn exec:java -Dexec.executable=\njava\n -DdisableRequestSignatureCheck=true -Dexec.args=$@\n\n\n\n\nThe service will listen on port 9999 by default.\n\n\nStart bst proxy\n\n\nOpen a new terminal and start the proxy for port 9999:\n\n\n$ bst proxy http 9999\n\n\n\n\nConfigure your Skill\n\n\nFrom the \nAlexa Skills Kit list\n within the Amazon Developer's Console:\n\n\nChoose \"Add a New Skill\"\n\n\nFill out the Information tab\n\n\n\n\nGive your skill a name and invocation phrase, 'bst java sample' and 'greeter' for example\n\n\n\n\nFill out the Interaction Model\n\n\n\n\nCopy and paste the Intent Schema from \nhere\n\n\nCopy and paste the Sample Utterances from \nhere\n\n\n\n\nConfigure the Endpoint\n\n\nWhen you started the proxy, bst printed out a URL that you need to configure your skill:\n\n\n$ bst proxy http 9999\nBST: v0.6.5  Node: v4.3.2\n\nYour URL for Alexa Skill configuration:\nhttps://proxy.bespoken.tools/YOUR/SKILL/PATH?node-id=0c6a4f17-c86f-4024-ba26-a351ac319431\n(Be sure to put in your real path and other query string parameters!)\n\n\n\n\n\nAlternatively, you can create this URL via the \nproxy urlgen\n command.\n\n\nYou first need to modify it to the path that your server is listening on, in this case it is \n/hello\n.\n\n\nhttps://proxy.bespoken.tools/hello?node-id=0c6a4f17-c86f-4024-ba26-a351ac319431\n\n\n\n\nCopy and paste this URL as your endpoint:\n\n\n\n\nAlso make sure you select \"HTTPS\" and account linking to \"NO\".\n\n\nConfigure SSL\n  \n\n\nOn the SSL Certificate page, select the middle radio button \"My development endpoint is a subdomain of a domain that has a wildcard certificate from a certificate authority\"\n\n\nTest\n\n\nGo to the service simulator, and type: \"hello\" and hit \"Ask \\\n\".\n\n\nYou should get a valid JSON in reply:\n\n\n\n\nNext Steps\n\n\nYou can now start adding functionality to your skill. To learn more about coding Alexa Skills, see the official \ndocumentation\n\n\nYou can also try it out on an Alexa device like an Echo, as long as it is registered with your account.\nJust say \"Open \\\n\" to use it.", 
            "title": "Local Java Server"
        }, 
        {
            "location": "/tutorials/tutorial_local_server_java/#prerequisites", 
            "text": "bespoken tools (bst)  $ npm install bespoken-tools -g  Installation Instructions    maven  OSX with homebrew:  $ brew install maven  Installation Instructions    Amazon Developer Account  Amazon Developer", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/tutorials/tutorial_local_server_java/#getting-started", 
            "text": "Clone the bst project:  $ git clone https://github.com/bespoken/bst  Go to the sample java skill  $ cd bst/samples/java", 
            "title": "Getting Started"
        }, 
        {
            "location": "/tutorials/tutorial_local_server_java/#run-the-sample-java-skill", 
            "text": "From within bst/samples/java directory, compile the example with this command:    $ mvn compile  Run the server with this command:    $ mvn exec:java -Dexec.executable= java  -DdisableRequestSignatureCheck=true -Dexec.args=$@  The service will listen on port 9999 by default.", 
            "title": "Run the Sample Java Skill"
        }, 
        {
            "location": "/tutorials/tutorial_local_server_java/#start-bst-proxy", 
            "text": "Open a new terminal and start the proxy for port 9999:  $ bst proxy http 9999", 
            "title": "Start bst proxy"
        }, 
        {
            "location": "/tutorials/tutorial_local_server_java/#configure-your-skill", 
            "text": "From the  Alexa Skills Kit list  within the Amazon Developer's Console:  Choose \"Add a New Skill\"  Fill out the Information tab   Give your skill a name and invocation phrase, 'bst java sample' and 'greeter' for example   Fill out the Interaction Model   Copy and paste the Intent Schema from  here  Copy and paste the Sample Utterances from  here   Configure the Endpoint  When you started the proxy, bst printed out a URL that you need to configure your skill:  $ bst proxy http 9999\nBST: v0.6.5  Node: v4.3.2\n\nYour URL for Alexa Skill configuration:\nhttps://proxy.bespoken.tools/YOUR/SKILL/PATH?node-id=0c6a4f17-c86f-4024-ba26-a351ac319431\n(Be sure to put in your real path and other query string parameters!)  Alternatively, you can create this URL via the  proxy urlgen  command.  You first need to modify it to the path that your server is listening on, in this case it is  /hello .  https://proxy.bespoken.tools/hello?node-id=0c6a4f17-c86f-4024-ba26-a351ac319431  Copy and paste this URL as your endpoint:   Also make sure you select \"HTTPS\" and account linking to \"NO\".  Configure SSL     On the SSL Certificate page, select the middle radio button \"My development endpoint is a subdomain of a domain that has a wildcard certificate from a certificate authority\"", 
            "title": "Configure your Skill"
        }, 
        {
            "location": "/tutorials/tutorial_local_server_java/#test", 
            "text": "Go to the service simulator, and type: \"hello\" and hit \"Ask \\ \".  You should get a valid JSON in reply:", 
            "title": "Test"
        }, 
        {
            "location": "/tutorials/tutorial_local_server_java/#next-steps", 
            "text": "You can now start adding functionality to your skill. To learn more about coding Alexa Skills, see the official  documentation  You can also try it out on an Alexa device like an Echo, as long as it is registered with your account.\nJust say \"Open \\ \" to use it.", 
            "title": "Next Steps"
        }, 
        {
            "location": "/tutorials/tutorial_flask_ask_python/", 
            "text": "This tutorial shows you how to get started developing with bst for Alexa Skills Kit in Python using \nFlask-Ask\n.\n\n\nPrerequisites\n\n\n\n\nBespoken command-line tools (bst)\n\n\n$ npm install bespoken-tools -g\n\n\nInstallation Instructions\n\n\n\n\n\n\nAmazon Developer Account\n\n\nAmazon Developer\n\n\n\n\n\n\nFlask-Ask\n\n\n$ pip install flask-ask\n\n\n\n\n\n\n\n\nGetting Started\n\n\nWe will use the tidepooler sample John \nprovides\n in the Flask-Ask repo..\n\n\nClone the repo:\n\n\n$ git clone https://github.com/johnwheeler/flask-ask.git\n\n\n\n\nAnd jump to the root of the tidepooler sample:\n\n\n$ cd flask-ask/samples/tidepooler/\n\n\n\n\nand start the local server for the skill:\n\n\n$ python tidepooler.py\n\n\n\n\nThe tidepooler skill is now running on your local machine, listening on port 5000.\n\n\nStart bst proxy\n\n\nOpen a new terminal and start the bst proxy:\n\n\n$ bst proxy http 5000\n\n\n\n\nwhere \nhttp\n is the protocol for the proxy and \n5000\n is the port the tidepooler skill server is listening on.\n\n\nConfigure your Skill\n\n\nFrom the \nAlexa Skills Kit list\n within the Amazon Developer's Console:\n\n\nChoose \"Add a New Skill\"\n\n\nFill out the Information tab\n\n\n\n\nGive your skill a name and invocation phrase, \"tidepooler\" and \"tidepooler\" for example\n\n\n\n\nFill out the Interaction Model\n\n\n\n\nCopy and paste the Intent Schema from \nhere\n\n\nClick \"Add Slot Type\", enter \nLIST_OF_CITIES\n as the type and copy and paste the values from \nhere\n, click \nSave\n\n\nClick \"Add Slot Type\", enter \nLIST_OF_STATES\n as the type and copy and paste the values from \nhere\n, click \nSave\n\n\nCopy and paste the Sample Utterances from \nhere\n\n\n\n\nConfigure the Endpoint\n\n\nWhen you started the proxy, bst printed out a URL that you need to configure your skill:\n\n\n$ bst proxy http 5000\nBST: v0.6.14  Node: v4.3.2\n\nYour URL for Alexa Skill configuration:\nhttps://proxy.bespoken.tools/YOUR/SKILL/PATH?node-id=d34ca5f3-f55e-4e07-9d1e-4fcf918433b5\n(Be sure to put in your real path and other query string parameters!)\n\nINFO  2016-09-12T17:34:52.202Z Connected - proxy.bespoken.tools:5000\n\n\n\n\nSince tidepooler.py does not have a path it is listening on, simply \n/\n, you can remove the \nYOUR/SKILL/PATH\n from the provided URL and copy and paste as your endpoint:\n\n\nhttps://proxy.bespoken.tools/?node-id=d34ca5f3-f55e-4e07-9d1e-4fcf918433b5\n\n\n\n\nAlso make sure you select \"HTTPS\" for the endpoint type and account linking is set to \"NO\".\n\n\nConfigure SSL\n  \n\n\nOn the SSL Certificate page, select the middle radio button \"My development endpoint is a subdomain of a domain that has a wildcard certificate from a certificate authority\"\n\n\nTest\n\n\nNow that you have the python server running, bst proxy running and your skill configured, it is time to test.  In the Service Simulator on the Test tab, try typing in some of the following phrases:\n\n\nget high tide\n\n\n\n\nwhen is the next highest water for virginia beach\n\n\n\n\nwhat cities are supported\n\n\n\n\nWe can also use the bst speak command to test locally instead of using the Service Simulator.  In order to do this, you need to tell Flask-Ask to not verify the request signatures (which it does by default).\n\n\nAfter \nline 55\n of tidepooler.py, insert the following line:\n\n\napp.config['ASK_VERIFY_REQUESTS'] = False\n\n\n\n\nPlease Note:\n  As mentioned in the \ndocumentation\n, this should be disabled for production.  \n\n\nRestart your python skill server and from a new terminal (make sure bst proxy is still running) at the root of the project run:\n\n\n$ bst speak -i speech_assets/IntentSchema.json -s speech_assets/SampleUtterances.txt get high tide\n\n\n\n\nYou should see the request and then the response back from tidepooler.py.\n\n\nYou can even test with slots by including your slot value in brackets, for example:\n\n\n$ bst speak -i speech_assets/IntentSchema.json -s speech_assets/SampleUtterances.txt when is the next highest water for {virginia beach}\n\n\n\n\nOther Resources\n\n\n\n\nbst proxy\n\n\nbst speak\n\n\nFlask-Ask Documentation\n\n\nFlask-Ask on Github\n\n\nFlask-Ask: A New Python Framework for Rapid Alexa Skills Kit Development", 
            "title": "Python & Flask-Ask"
        }, 
        {
            "location": "/tutorials/tutorial_flask_ask_python/#prerequisites", 
            "text": "Bespoken command-line tools (bst)  $ npm install bespoken-tools -g  Installation Instructions    Amazon Developer Account  Amazon Developer    Flask-Ask  $ pip install flask-ask", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/tutorials/tutorial_flask_ask_python/#getting-started", 
            "text": "We will use the tidepooler sample John  provides  in the Flask-Ask repo..  Clone the repo:  $ git clone https://github.com/johnwheeler/flask-ask.git  And jump to the root of the tidepooler sample:  $ cd flask-ask/samples/tidepooler/  and start the local server for the skill:  $ python tidepooler.py  The tidepooler skill is now running on your local machine, listening on port 5000.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/tutorials/tutorial_flask_ask_python/#start-bst-proxy", 
            "text": "Open a new terminal and start the bst proxy:  $ bst proxy http 5000  where  http  is the protocol for the proxy and  5000  is the port the tidepooler skill server is listening on.", 
            "title": "Start bst proxy"
        }, 
        {
            "location": "/tutorials/tutorial_flask_ask_python/#configure-your-skill", 
            "text": "From the  Alexa Skills Kit list  within the Amazon Developer's Console:  Choose \"Add a New Skill\"  Fill out the Information tab   Give your skill a name and invocation phrase, \"tidepooler\" and \"tidepooler\" for example   Fill out the Interaction Model   Copy and paste the Intent Schema from  here  Click \"Add Slot Type\", enter  LIST_OF_CITIES  as the type and copy and paste the values from  here , click  Save  Click \"Add Slot Type\", enter  LIST_OF_STATES  as the type and copy and paste the values from  here , click  Save  Copy and paste the Sample Utterances from  here   Configure the Endpoint  When you started the proxy, bst printed out a URL that you need to configure your skill:  $ bst proxy http 5000\nBST: v0.6.14  Node: v4.3.2\n\nYour URL for Alexa Skill configuration:\nhttps://proxy.bespoken.tools/YOUR/SKILL/PATH?node-id=d34ca5f3-f55e-4e07-9d1e-4fcf918433b5\n(Be sure to put in your real path and other query string parameters!)\n\nINFO  2016-09-12T17:34:52.202Z Connected - proxy.bespoken.tools:5000  Since tidepooler.py does not have a path it is listening on, simply  / , you can remove the  YOUR/SKILL/PATH  from the provided URL and copy and paste as your endpoint:  https://proxy.bespoken.tools/?node-id=d34ca5f3-f55e-4e07-9d1e-4fcf918433b5  Also make sure you select \"HTTPS\" for the endpoint type and account linking is set to \"NO\".  Configure SSL     On the SSL Certificate page, select the middle radio button \"My development endpoint is a subdomain of a domain that has a wildcard certificate from a certificate authority\"", 
            "title": "Configure your Skill"
        }, 
        {
            "location": "/tutorials/tutorial_flask_ask_python/#test", 
            "text": "Now that you have the python server running, bst proxy running and your skill configured, it is time to test.  In the Service Simulator on the Test tab, try typing in some of the following phrases:  get high tide  when is the next highest water for virginia beach  what cities are supported  We can also use the bst speak command to test locally instead of using the Service Simulator.  In order to do this, you need to tell Flask-Ask to not verify the request signatures (which it does by default).  After  line 55  of tidepooler.py, insert the following line:  app.config['ASK_VERIFY_REQUESTS'] = False  Please Note:   As mentioned in the  documentation , this should be disabled for production.    Restart your python skill server and from a new terminal (make sure bst proxy is still running) at the root of the project run:  $ bst speak -i speech_assets/IntentSchema.json -s speech_assets/SampleUtterances.txt get high tide  You should see the request and then the response back from tidepooler.py.  You can even test with slots by including your slot value in brackets, for example:  $ bst speak -i speech_assets/IntentSchema.json -s speech_assets/SampleUtterances.txt when is the next highest water for {virginia beach}", 
            "title": "Test"
        }, 
        {
            "location": "/tutorials/tutorial_flask_ask_python/#other-resources", 
            "text": "bst proxy  bst speak  Flask-Ask Documentation  Flask-Ask on Github  Flask-Ask: A New Python Framework for Rapid Alexa Skills Kit Development", 
            "title": "Other Resources"
        }, 
        {
            "location": "/tutorials/tutorial_bst_emulator_nodejs/", 
            "text": "This tutorial shows you how to get started using our BST Alexa emulator with Node.js and Javascript.\n\n\nThe purpose of the emulator is to enable unit- and functional-testing of Alexa Skills, \nallowing one to:\n\n\n\n\nEmulate the complex behavior of the Alexa service, without an Alexa device or any manual interaction\n\n\nEnsure ongoing skill quality by embedding tests in Continuous Integration/Deployment processes\n\n\n\n\nAnd please note - though this example is written in Javascript, the emulator can be used to test non-Javascript-based skills!\n\n\nAdditionally, though this example focuses on the AudioPlayer interface, \nthe BSTAlexa emulator can be used for testing regular Alexa skills as well.\n\n\nTutorial Prerequisites\n\n\n\n\nMocha Test Framework\n\n\nhttps://mochajs.org/#getting-started\n\n\n$ npm install mocha --save-dev\n\n\n\n\n\n\nA Node.js, Lambda-based Alexa skill \n\n\nIf you do not have one and want to follow along at home, \ntry ours here\n.\n\n\nDerived from excellent streaming skill example provided by Amazon.\n\n\nThe test used in this tutorial is \nfound here\n.\n\n\n\n\n\n\nBespoken tools added to your project's package.json\n\n\n$ npm install bespoken-tools --save-dev\n\n\nFor this example, we make it a \"dev\" dependency as we will be using it only for testing.\n\n\n\n\n\n\n\n\nTest Structure\n\n\nWe are using Mocha for tests. We breakdown our tests into three parts:\n\n\n\n\nbeforeEach - Creates a local Lambda server and initializes the emulator\n\n\ntests - Performs the actual tests by issuing intents and emulating AudioPlayer behavior\n\n\nafterEach - Tears down the local Lambda server and shuts down the emulator\n\n\n\n\nThese are standard testing considerations, \nthough we emphasize the importance of stopping the Lambda server.\n\nIf this is not done, it will continue listening on the assigned port, potentially causing conflicts and errors.\n\n\nAdding the bst module\n\n\nAt the top of your test, include:\n\n\nvar bst = require('bespoken-tools');\n\n\n\n\nSetup and Teardown\n\n\nvar server = null;\nvar alexa = null;\n\nbeforeEach(function (done) {\n    server = new bst.LambdaServer('./index.js', 10000, true);\n    alexa = new bst.BSTAlexa('http://localhost:10000',\n                             '../speechAssets/IntentSchema.json',\n                             '../speechAssets/Utterances.txt');\n    server.start(function() {\n        alexa.start(function (error) {\n            if (error !== undefined) {\n                console.error(\nError: \n + error);\n            } else {\n                done();\n            }\n        });\n    });\n});\n\nafterEach(function(done) {\n    alexa.stop(function () {\n        server.stop(function () {\n            done();\n        });\n    });\n});\n\n\n\n\nThe beforeEach block initializes the \nLambdaServer\n \nand the \nBSTAlexa emulator\n.\n\n\nThe first parameter to the LambdaServer tells it the location of the Lambda file to be run.\nIt automatically wraps the Lambda in a service, and exposes it on the port supplied in the second argument.\n\n\nThe last parameter indicates it should be in verbose mode - this causes requests and responses from the skill to be printed to the console.\n\n\nThe \nBSTAlexa start call\n takes the URL of the Lambda server as its first parameter (it begins listening).\n\n\nNote that if you are not using Lambdas but a \"Plain Old HTTP Service\" you can skip the server start call and just point it at a server here.\n\n\nAdditionally, it takes the location of the Intent Schema and Utterances in the second and third parameters. \nThese are not required and will default to './speechAssets/IntentSchema.json' and './speechAssets/SampleUtterances.txt' respectively.\n\n\nThe afterEach block ensures the LambdaServer and BSTAlexa emulator are shutdown correctly.\n\n\nFirst Simple Test\n\n\nit('Launches and then plays first', function (done) {\n    // Launch the skill via sending it a LaunchRequest\n    alexa.launched(function (error, payload) {\n        // Check that the introduction is play as outputSpeech\n        assert.equal(payload.response.outputSpeech.ssml, '\nspeak\n \naudio src=\nhttps://s3.amazonaws.com/bespoken/streaming/bespokenspodcast-INTRODUCTION.mp3\n /\nYou can say play, scan titles, or about the podcast \n/speak\n');\n\n        // Emulate the user saying 'Play'\n        alexa.spoken('Play', function (error, payload) {\n            // Ensure the correct directive and audioItem is returned\n            assert.equal(payload.response.directives[0].type, 'AudioPlayer.Play');\n            assert.equal(payload.response.directives[0].audioItem.stream.token, '0');\n            assert.equal(payload.response.directives[0].audioItem.stream.url, 'https://traffic.libsyn.com/bespoken/TIP103.mp3?dest-id=432208');\n            done();\n        });\n    });\n});\n\n\n\n\nThis test runs through some simple behavior:\n\n\n\n\nIt emulates the Skill being launched\n\n\nIt confirms the Skill returns the correct outputSpeech after being launched\n\n\nIt emulates the user saying 'Play'\n\n\nIt confirms the correct directive and AudioItem is returned for the 'Play' intent\n\n\n\n\nThe goal is to test until we feel confident in the behavior of our skill, and that it is correctly handling the interaction with the Alexa Service.\n\n\nIt is a straightforward exercise to add more property checks on the payload to further confirm behavior.\n\n\nA More Complex Test\n\n\nit('Plays To Completion', function (done) {\n    alexa.spoken('Play', function (error, payload) {\n        // Emulates the track being played 'NearlyFinished'\n        //  Alexa sends this event at some point during track playback\n        // Our skill uses the opportunity to queue up the next track to play\n        alexa.playbackNearlyFinished(function (error, payload) {\n            assert.equal(payload.response.directives[0].type, 'AudioPlayer.Play');\n            assert.equal(payload.response.directives[0].playBehavior, 'ENQUEUE');\n            assert.equal(payload.response.directives[0].audioItem.stream.url, 'https://traffic.libsyn.com/bespoken/TIP104.mp3?dest-id=432208');\n        });\n\n        // Emulates the track playing to completion\n        // The callback is invoked after the skill responds to the PlaybackFinished request\n        alexa.playbackFinished(function (error, payload) {\n            // Confirm there are no directives in the reply to the PlaybackFinished request\n            // They came on the PlaybackNearlyFinished call\n            assert(!payload.response.directives);\n\n            // Check that playback started on the next track\n            alexa.once('AudioPlayer.PlaybackStarted', function(audioItem) {\n                assert.equal(audioItem.stream.token, '1');\n                assert.equal(audioItem.stream.url, 'https://traffic.libsyn.com/bespoken/TIP104.mp3?dest-id=432208');\n                done();\n            });\n        });\n    });\n});\n\n\n\n\nThis test uses the \nBSTAlexa#playbackFinished() call\n \nto emulate the audio playing to completion on the device.  \n\n\nThe Alexa service first calls \nBSTAlexa#playbackNearlyFinished()\n. \nThis request is triggered by the Alexa service when a track is almost done playing, and is frequently used by skills to enqueue the next AudioItem in the queue for playback on the device.\n\n\nThe Alexa service then sends a 'AudioPlayer.PlaybackFinished' request to the skill, which we expect to then trigger the playback of the next track in the queue.  \n\n\nWe also use the \nBSTAlexa#once() listener\n - this allows us to listen for specific events occurring within the Alexa emulator. \nIn this case, we want to confirm that the next track was queued correctly and has begun playing.\n\n\nWe use the once call to indicate we only want to receive this event the first time it happens. This is useful for watching on events like PlaybackStarted, which are likely to happen many times in the course of an interaction.\n\n\nThe events that can be listened for are listed \nhere\n.\n\n\nGoing Even Further\n\n\nOur sample project, \nBespoken Streamer\n, provides even more examples.\n\n\nThe \ntests for it\n are meant to exercise all the different actions\nand states that the skill allows for. \n\n\nIt gets quite involved, and the complexity really illustrates the need for such a tool: without it, manually working through each of these scenarios initially is daunting.\n\n\nAnd ensuring all the scenarios still work when changes are made to the code is even more challenging. \n\n\nIt's essential to have a unit-test tool such as this in one's toolbelt to avoid being plagued with quality issues.\n\n\nParting Words\n\n\nWe are looking to continuously enhance the emulator. Right now, it supports:\n\n\n\n\nStandard Custom Skill behavior\n\n\nAudioPlayer behavior\n\n\n\n\nIt does \nnot\n yet support:\n\n\n\n\nError conditions (\nAlexa requests based on improper Skill responses\n)\n\n\nThe PlaybackController interface\n\n\n\n\nPlease keep a lookout for support for both coming soon!\n\n\nAdditionally, \nchat with us on Gitter\n with any comments or questions.", 
            "title": "Alexa Emulator & Node.js"
        }, 
        {
            "location": "/tutorials/tutorial_bst_emulator_nodejs/#tutorial-prerequisites", 
            "text": "Mocha Test Framework  https://mochajs.org/#getting-started  $ npm install mocha --save-dev    A Node.js, Lambda-based Alexa skill   If you do not have one and want to follow along at home,  try ours here .  Derived from excellent streaming skill example provided by Amazon.  The test used in this tutorial is  found here .    Bespoken tools added to your project's package.json  $ npm install bespoken-tools --save-dev  For this example, we make it a \"dev\" dependency as we will be using it only for testing.", 
            "title": "Tutorial Prerequisites"
        }, 
        {
            "location": "/tutorials/tutorial_bst_emulator_nodejs/#test-structure", 
            "text": "We are using Mocha for tests. We breakdown our tests into three parts:   beforeEach - Creates a local Lambda server and initializes the emulator  tests - Performs the actual tests by issuing intents and emulating AudioPlayer behavior  afterEach - Tears down the local Lambda server and shuts down the emulator   These are standard testing considerations,  though we emphasize the importance of stopping the Lambda server. \nIf this is not done, it will continue listening on the assigned port, potentially causing conflicts and errors.", 
            "title": "Test Structure"
        }, 
        {
            "location": "/tutorials/tutorial_bst_emulator_nodejs/#adding-the-bst-module", 
            "text": "At the top of your test, include:  var bst = require('bespoken-tools');", 
            "title": "Adding the bst module"
        }, 
        {
            "location": "/tutorials/tutorial_bst_emulator_nodejs/#setup-and-teardown", 
            "text": "var server = null;\nvar alexa = null;\n\nbeforeEach(function (done) {\n    server = new bst.LambdaServer('./index.js', 10000, true);\n    alexa = new bst.BSTAlexa('http://localhost:10000',\n                             '../speechAssets/IntentSchema.json',\n                             '../speechAssets/Utterances.txt');\n    server.start(function() {\n        alexa.start(function (error) {\n            if (error !== undefined) {\n                console.error( Error:   + error);\n            } else {\n                done();\n            }\n        });\n    });\n});\n\nafterEach(function(done) {\n    alexa.stop(function () {\n        server.stop(function () {\n            done();\n        });\n    });\n});  The beforeEach block initializes the  LambdaServer  \nand the  BSTAlexa emulator .  The first parameter to the LambdaServer tells it the location of the Lambda file to be run.\nIt automatically wraps the Lambda in a service, and exposes it on the port supplied in the second argument.  The last parameter indicates it should be in verbose mode - this causes requests and responses from the skill to be printed to the console.  The  BSTAlexa start call  takes the URL of the Lambda server as its first parameter (it begins listening).  Note that if you are not using Lambdas but a \"Plain Old HTTP Service\" you can skip the server start call and just point it at a server here.  Additionally, it takes the location of the Intent Schema and Utterances in the second and third parameters. \nThese are not required and will default to './speechAssets/IntentSchema.json' and './speechAssets/SampleUtterances.txt' respectively.  The afterEach block ensures the LambdaServer and BSTAlexa emulator are shutdown correctly.", 
            "title": "Setup and Teardown"
        }, 
        {
            "location": "/tutorials/tutorial_bst_emulator_nodejs/#first-simple-test", 
            "text": "it('Launches and then plays first', function (done) {\n    // Launch the skill via sending it a LaunchRequest\n    alexa.launched(function (error, payload) {\n        // Check that the introduction is play as outputSpeech\n        assert.equal(payload.response.outputSpeech.ssml, ' speak   audio src= https://s3.amazonaws.com/bespoken/streaming/bespokenspodcast-INTRODUCTION.mp3  / You can say play, scan titles, or about the podcast  /speak ');\n\n        // Emulate the user saying 'Play'\n        alexa.spoken('Play', function (error, payload) {\n            // Ensure the correct directive and audioItem is returned\n            assert.equal(payload.response.directives[0].type, 'AudioPlayer.Play');\n            assert.equal(payload.response.directives[0].audioItem.stream.token, '0');\n            assert.equal(payload.response.directives[0].audioItem.stream.url, 'https://traffic.libsyn.com/bespoken/TIP103.mp3?dest-id=432208');\n            done();\n        });\n    });\n});  This test runs through some simple behavior:   It emulates the Skill being launched  It confirms the Skill returns the correct outputSpeech after being launched  It emulates the user saying 'Play'  It confirms the correct directive and AudioItem is returned for the 'Play' intent   The goal is to test until we feel confident in the behavior of our skill, and that it is correctly handling the interaction with the Alexa Service.  It is a straightforward exercise to add more property checks on the payload to further confirm behavior.", 
            "title": "First Simple Test"
        }, 
        {
            "location": "/tutorials/tutorial_bst_emulator_nodejs/#a-more-complex-test", 
            "text": "it('Plays To Completion', function (done) {\n    alexa.spoken('Play', function (error, payload) {\n        // Emulates the track being played 'NearlyFinished'\n        //  Alexa sends this event at some point during track playback\n        // Our skill uses the opportunity to queue up the next track to play\n        alexa.playbackNearlyFinished(function (error, payload) {\n            assert.equal(payload.response.directives[0].type, 'AudioPlayer.Play');\n            assert.equal(payload.response.directives[0].playBehavior, 'ENQUEUE');\n            assert.equal(payload.response.directives[0].audioItem.stream.url, 'https://traffic.libsyn.com/bespoken/TIP104.mp3?dest-id=432208');\n        });\n\n        // Emulates the track playing to completion\n        // The callback is invoked after the skill responds to the PlaybackFinished request\n        alexa.playbackFinished(function (error, payload) {\n            // Confirm there are no directives in the reply to the PlaybackFinished request\n            // They came on the PlaybackNearlyFinished call\n            assert(!payload.response.directives);\n\n            // Check that playback started on the next track\n            alexa.once('AudioPlayer.PlaybackStarted', function(audioItem) {\n                assert.equal(audioItem.stream.token, '1');\n                assert.equal(audioItem.stream.url, 'https://traffic.libsyn.com/bespoken/TIP104.mp3?dest-id=432208');\n                done();\n            });\n        });\n    });\n});  This test uses the  BSTAlexa#playbackFinished() call  \nto emulate the audio playing to completion on the device.    The Alexa service first calls  BSTAlexa#playbackNearlyFinished() . \nThis request is triggered by the Alexa service when a track is almost done playing, and is frequently used by skills to enqueue the next AudioItem in the queue for playback on the device.  The Alexa service then sends a 'AudioPlayer.PlaybackFinished' request to the skill, which we expect to then trigger the playback of the next track in the queue.    We also use the  BSTAlexa#once() listener  - this allows us to listen for specific events occurring within the Alexa emulator. \nIn this case, we want to confirm that the next track was queued correctly and has begun playing.  We use the once call to indicate we only want to receive this event the first time it happens. This is useful for watching on events like PlaybackStarted, which are likely to happen many times in the course of an interaction.  The events that can be listened for are listed  here .", 
            "title": "A More Complex Test"
        }, 
        {
            "location": "/tutorials/tutorial_bst_emulator_nodejs/#going-even-further", 
            "text": "Our sample project,  Bespoken Streamer , provides even more examples.  The  tests for it  are meant to exercise all the different actions\nand states that the skill allows for.   It gets quite involved, and the complexity really illustrates the need for such a tool: without it, manually working through each of these scenarios initially is daunting.  And ensuring all the scenarios still work when changes are made to the code is even more challenging.   It's essential to have a unit-test tool such as this in one's toolbelt to avoid being plagued with quality issues.", 
            "title": "Going Even Further"
        }, 
        {
            "location": "/tutorials/tutorial_bst_emulator_nodejs/#parting-words", 
            "text": "We are looking to continuously enhance the emulator. Right now, it supports:   Standard Custom Skill behavior  AudioPlayer behavior   It does  not  yet support:   Error conditions ( Alexa requests based on improper Skill responses )  The PlaybackController interface   Please keep a lookout for support for both coming soon!  Additionally,  chat with us on Gitter  with any comments or questions.", 
            "title": "Parting Words"
        }, 
        {
            "location": "/tutorials/tutorial_cloud_function/", 
            "text": "This tutorial shows you how to get started developing for Actions on Google using a Google Cloud Function with Bespoken.\n\n\nPrerequisites\n\n\n\n\nBespoken command-line tools (bst)\n\n\n$ npm install bespoken-tools -g\n\n\nInstallation Instructions\n\n\n\n\n\n\nGoogle Cloud account\n\n\nGoogle Cloud\n\n\n\n\n\n\nAPI.AI account\n\n\nAPI.AI\n\n\n\n\n\n\n\n\nGetting Started\n\n\nClone the Super Simple Google Action repo:\n\n\n$ git clone https://github.com/bespoken/super-simple-google-action\n\n\n\n\nGo to the root level of the sample:\n\n\n$ cd super-simple-google-action/\n\n\n\n\nInstall the dependencies:\n\n\n$ npm install\n\n\n\n\nConfigure your API.AI Action\n\n\nFor a detailed walkthrough on setting up an Action on Google with API.AI, go \nhere\n.\n\n\nStart bst proxy\n\n\nFor Google Cloud Functions, the \nbst proxy\n command, in addition to setting up the proxy, will run your function for you and even reload it on changes.\n\n\nThis will start the function:\n\n\n$ bst proxy function index.js simpleFunction\n\n\n\n\nTry it out\n\n\nYou can test things out right inside API.AI - just enter \"Hello\" into the \"Try it now\" field on the top-right.\n\n\nYou should see the request and response come across the console where the \nbst proxy\n is running, like so:\n\n\n\n\nOr you can try it in the \nActions on Google Web Simulator\n.\n\n\nHooking into the Bespoken Dashboard\n\n\nTo use our monitoring and logging facility, \nsign up here\n.\n\n\nOnce you have signed in, create a new source by clicking on the \"+\" button at the bottom or link at the top:\n\n\n\nName your source and then hit \"Create Source\". On the following page, select \"Next: Check For Logs\".\n\n\nOn the right-hand side of the page, select \"Show\" over the Secret Key:\n\n\n\nCut and paste the secret key into the index.js file in Super Simple Google Action project:\n\n\n\nThe line is at the bottom of the file.\n\n\nNow, the summary and log data for your action will be available in the Dashboard, both while using the proxy for development and once you go live!\n\n\n\nNext Steps\n\n\nYou can now start adding functionality to your action. To learn more about working with Actions on Google, see the official \ndocumentation\n\n\nYou can also try it out on a Google Home or \nGoogle Assistant-enabled phone\n, as long as it is registered with your account.\nJust say \"Talk To {Your Invocation Name}\" to use it.", 
            "title": "With Cloud Functions"
        }, 
        {
            "location": "/tutorials/tutorial_cloud_function/#prerequisites", 
            "text": "Bespoken command-line tools (bst)  $ npm install bespoken-tools -g  Installation Instructions    Google Cloud account  Google Cloud    API.AI account  API.AI", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/tutorials/tutorial_cloud_function/#getting-started", 
            "text": "Clone the Super Simple Google Action repo:  $ git clone https://github.com/bespoken/super-simple-google-action  Go to the root level of the sample:  $ cd super-simple-google-action/  Install the dependencies:  $ npm install", 
            "title": "Getting Started"
        }, 
        {
            "location": "/tutorials/tutorial_cloud_function/#configure-your-apiai-action", 
            "text": "For a detailed walkthrough on setting up an Action on Google with API.AI, go  here .", 
            "title": "Configure your API.AI Action"
        }, 
        {
            "location": "/tutorials/tutorial_cloud_function/#start-bst-proxy", 
            "text": "For Google Cloud Functions, the  bst proxy  command, in addition to setting up the proxy, will run your function for you and even reload it on changes.  This will start the function:  $ bst proxy function index.js simpleFunction", 
            "title": "Start bst proxy"
        }, 
        {
            "location": "/tutorials/tutorial_cloud_function/#try-it-out", 
            "text": "You can test things out right inside API.AI - just enter \"Hello\" into the \"Try it now\" field on the top-right.  You should see the request and response come across the console where the  bst proxy  is running, like so:   Or you can try it in the  Actions on Google Web Simulator .", 
            "title": "Try it out"
        }, 
        {
            "location": "/tutorials/tutorial_cloud_function/#hooking-into-the-bespoken-dashboard", 
            "text": "To use our monitoring and logging facility,  sign up here .  Once you have signed in, create a new source by clicking on the \"+\" button at the bottom or link at the top:  Name your source and then hit \"Create Source\". On the following page, select \"Next: Check For Logs\".  On the right-hand side of the page, select \"Show\" over the Secret Key:  Cut and paste the secret key into the index.js file in Super Simple Google Action project:  The line is at the bottom of the file.  Now, the summary and log data for your action will be available in the Dashboard, both while using the proxy for development and once you go live!", 
            "title": "Hooking into the Bespoken Dashboard"
        }, 
        {
            "location": "/tutorials/tutorial_cloud_function/#next-steps", 
            "text": "You can now start adding functionality to your action. To learn more about working with Actions on Google, see the official  documentation  You can also try it out on a Google Home or  Google Assistant-enabled phone , as long as it is registered with your account.\nJust say \"Talk To {Your Invocation Name}\" to use it.", 
            "title": "Next Steps"
        }, 
        {
            "location": "/tutorials/tutorial_configuring_api_ai/", 
            "text": "This tutorial will help you get setup using API.AI with our sample project and Bespoken.\n\n\nPing us on \ngitter\n if you have any question.\n\n\nThis tutorial goes great with our \nBespoken for Cloud Functions tutorial\n :-)\n\n\nPre-Requisites\n\n\nThis tutorial assumes you have cloned the \nSuper Simple Google Action repository\n.\n\n\nIf you have not already, do it with this command:\n\n\ngit clone https://github.com/bespoken/super-simple-google-action\n\n\n\n\nWalk-through\n\n\nRegister/login to API.AI account\n\n\nGo to the \nAPI.AI website\n:\n\n\n\n\nSign-in or create your account.\n\n\nCreate a new agent\n\n\n\n\nClick on the \"Create Agent\" button.\n\n\nName the agent\n\n\n\n\nGive the agent a name, then click \"Save\".\n\n\nAfter clicking save, click the \"Export and Import\" tab.\n\n\nRestore sample action\n\n\nSelect \"Restore From Zip\":\n\n\n\n\nClick \"Select File\".\n\n\nBrowse to the directory where you cloned the \nSuper Simple Google Action repository\n:\n\n\n\n\nChoose \"SuperSimpleGoogleAction.zip\" from the file picker and click \"Open\".\n\n\nIf you the click on \"Intents\" on the left-hand side menu, you should now see the following:\n\n\n\n\nThose intents will be available if the agent was properly restored.\n\n\nEnable Webhook Fulfillment\n\n\nClick on Fulfillment from the left-hand menu.\n\n\nToggle \"Enabled\" so that Webhook fulfillment is enabled:\n\n\n\n\nThis will cause the configured Intents to call out the specified URL for handling user requests.\n\n\nThe URL should be your \nbst proxy\n endpoint. To find it, just type at the command-line:\n\n\nbst proxy function index.js simpleFunction\n\n\n\n\nGrab the URL from the console output, which should look like this:\n\n\n\n\nAfter entering this, click \"Save\" at the top.\n\n\nEnable Actions on Google integration\n\n\nSelect the \"Integrations\" tab on the left.\n\n\n\n\nEnable the first option, \"Actions on Google\".\n\n\n\n\nEnter the \"World\" for the \"Invocation Name\". If you have a Google Cloud Project setup, enter the ID on this screen - \nyou can find the project ID here\n.\n\n\nIf you want to use this Action on your Google Home device, \nGoogle Assistant-enabled phone\n or in the \nWeb Simulator\n, click \"Preview\".\n\n\nNext Steps\n\n\nTo use this API.AI-based Action, follow our \ntutorial here\n.", 
            "title": "With API.AI"
        }, 
        {
            "location": "/tutorials/tutorial_configuring_api_ai/#pre-requisites", 
            "text": "This tutorial assumes you have cloned the  Super Simple Google Action repository .  If you have not already, do it with this command:  git clone https://github.com/bespoken/super-simple-google-action", 
            "title": "Pre-Requisites"
        }, 
        {
            "location": "/tutorials/tutorial_configuring_api_ai/#walk-through", 
            "text": "", 
            "title": "Walk-through"
        }, 
        {
            "location": "/tutorials/tutorial_configuring_api_ai/#registerlogin-to-apiai-account", 
            "text": "Go to the  API.AI website :   Sign-in or create your account.", 
            "title": "Register/login to API.AI account"
        }, 
        {
            "location": "/tutorials/tutorial_configuring_api_ai/#create-a-new-agent", 
            "text": "Click on the \"Create Agent\" button.", 
            "title": "Create a new agent"
        }, 
        {
            "location": "/tutorials/tutorial_configuring_api_ai/#name-the-agent", 
            "text": "Give the agent a name, then click \"Save\".  After clicking save, click the \"Export and Import\" tab.", 
            "title": "Name the agent"
        }, 
        {
            "location": "/tutorials/tutorial_configuring_api_ai/#restore-sample-action", 
            "text": "Select \"Restore From Zip\":   Click \"Select File\".  Browse to the directory where you cloned the  Super Simple Google Action repository :   Choose \"SuperSimpleGoogleAction.zip\" from the file picker and click \"Open\".  If you the click on \"Intents\" on the left-hand side menu, you should now see the following:   Those intents will be available if the agent was properly restored.", 
            "title": "Restore sample action"
        }, 
        {
            "location": "/tutorials/tutorial_configuring_api_ai/#enable-webhook-fulfillment", 
            "text": "Click on Fulfillment from the left-hand menu.  Toggle \"Enabled\" so that Webhook fulfillment is enabled:   This will cause the configured Intents to call out the specified URL for handling user requests.  The URL should be your  bst proxy  endpoint. To find it, just type at the command-line:  bst proxy function index.js simpleFunction  Grab the URL from the console output, which should look like this:   After entering this, click \"Save\" at the top.", 
            "title": "Enable Webhook Fulfillment"
        }, 
        {
            "location": "/tutorials/tutorial_configuring_api_ai/#enable-actions-on-google-integration", 
            "text": "Select the \"Integrations\" tab on the left.   Enable the first option, \"Actions on Google\".   Enter the \"World\" for the \"Invocation Name\". If you have a Google Cloud Project setup, enter the ID on this screen -  you can find the project ID here .  If you want to use this Action on your Google Home device,  Google Assistant-enabled phone  or in the  Web Simulator , click \"Preview\".", 
            "title": "Enable Actions on Google integration"
        }, 
        {
            "location": "/tutorials/tutorial_configuring_api_ai/#next-steps", 
            "text": "To use this API.AI-based Action, follow our  tutorial here .", 
            "title": "Next Steps"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_local/", 
            "text": "This tutorial shows you how to use Bespoken to test AWS Lambdas locally.\n\n\nUsing this, you can send HTTP requests directly to and from your AWS Lambda (Node.js or Python) on your laptop.\n\n\nPrerequisites\n\n\nFor Node.js - Install bst:\n\n\n\n\n$ npm install bespoken-tools -g\n\n\nFull Installation Instructions\n\n\n\n\nFor Python - Install bstpy:\n\n\n\n\ngit clone the bstpy repo\n\n\ncd bstpy\n\n\npip install -e bstpy\n\n\nFull Installation Instructions\n\n\n\n\nCreating The HTTP Service\n\n\nThese examples are based on our own sample project (which in turn is based on Amazon's HelloWorld Lambda examples). If  you want to use our sample project, just clone this repo:\n\n\ngit clone https://github.com/bespoken/lambda_samples\n\n\n\n\nChange directories to the cloned project:\n\n\ncd lambda_samples\n\n\n\n\nOf course, feel free to use your own existing Lambda project.\n\n\nExposing the Node.js Lambda as an HTTP service\n\nRun the \nbst proxy lambda\n command to do this:\n\n\nbst proxy lambda HelloWorld.js --verbose\n\n\n\n\nThe --verbose option will cause additional useful information to be printed to the console.\n\n\nExposing the Python Lambda as an HTTP service\n\nRun the \nbstpy\n command to do this:\n\n\nbstpy HelloWorld.handler\n\n\n\n\nIn both cases, the Lambda will be exposed as an HTTP service at http://localhost:10000.\n\n\nTesting The HTTP Service\n\n\nUsing curl (from another terminal window, or from the same terminal by running \nbst\n or \nbstpy\n in the background):\n\n\ncurl -H \nContent-Type: application/json\n \\\n  -X POST \\\n  -d '{\nkey1\n:\nvalue1\n,\nkey2\n:\nvalue2\n, \nkey3\n: \nvalue3\n}' \\\n  http://localhost:10000\n\n\n\n\nAnd the output:\n\n\n\n\nThe actual output from the Lambda is highlighted in red.\n\n\nObviously, much more complex Lambdas and responses are possible. With this complexity, being able to test them locally like this only becomes more vital.\n\n\nNote\n - we recommend using Postman for these sort of manual tests - it provides essentially a nice UI for curl:\n\nGet Postman\n.\n\n\nMore Neat Stuff\n\n\nThis is meant to be a basic demonstration, but there is much more that you can do:  \n\n\nAuto-reload\n\nWhen you make changes to your lambda code, \nbst\n and \nbstpy\n will automatically re-load the changes without restarting the server.\n\n\nDebugging\n\nYou can step through your code using your favorite IDE. \nFollow this tutorial\n to learn how easy this is to setup.\n\n\nWebhooks\n\nUsing the URL printed out when the \nbst proxy\n starts up, you can access your local service from across the web.\n\n\n\n\nTake the URL that is printed out for you and see that you can hit it from anywhere on the web.\n\n\nThis is extremely useful for debugging Webhook-based services (like Facebook Messenger Platform, Alexa Skills, Slack Webhooks, Microsoft Bot framework, etc.) that require having a public-facing server.\n\n\nDeployment\n\nFor Node.js Lambdas, we have a one-step deploy tool. You can \nread about it here\n.\n\n\nAlexa Skills\n\nWe have built this originally to make it super-easy to develop Alexa skills. We offer an \narray of commands and APIs\n that aim to make developing for Alexa faster and more bullet-proof.", 
            "title": "Running Lambdas Locally"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_local/#prerequisites", 
            "text": "For Node.js - Install bst:   $ npm install bespoken-tools -g  Full Installation Instructions   For Python - Install bstpy:   git clone the bstpy repo  cd bstpy  pip install -e bstpy  Full Installation Instructions", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_local/#creating-the-http-service", 
            "text": "These examples are based on our own sample project (which in turn is based on Amazon's HelloWorld Lambda examples). If  you want to use our sample project, just clone this repo:  git clone https://github.com/bespoken/lambda_samples  Change directories to the cloned project:  cd lambda_samples  Of course, feel free to use your own existing Lambda project.  Exposing the Node.js Lambda as an HTTP service \nRun the  bst proxy lambda  command to do this:  bst proxy lambda HelloWorld.js --verbose  The --verbose option will cause additional useful information to be printed to the console.  Exposing the Python Lambda as an HTTP service \nRun the  bstpy  command to do this:  bstpy HelloWorld.handler  In both cases, the Lambda will be exposed as an HTTP service at http://localhost:10000.", 
            "title": "Creating The HTTP Service"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_local/#testing-the-http-service", 
            "text": "Using curl (from another terminal window, or from the same terminal by running  bst  or  bstpy  in the background):  curl -H  Content-Type: application/json  \\\n  -X POST \\\n  -d '{ key1 : value1 , key2 : value2 ,  key3 :  value3 }' \\\n  http://localhost:10000  And the output:   The actual output from the Lambda is highlighted in red.  Obviously, much more complex Lambdas and responses are possible. With this complexity, being able to test them locally like this only becomes more vital.  Note  - we recommend using Postman for these sort of manual tests - it provides essentially a nice UI for curl: Get Postman .", 
            "title": "Testing The HTTP Service"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_local/#more-neat-stuff", 
            "text": "This is meant to be a basic demonstration, but there is much more that you can do:    Auto-reload \nWhen you make changes to your lambda code,  bst  and  bstpy  will automatically re-load the changes without restarting the server.  Debugging \nYou can step through your code using your favorite IDE.  Follow this tutorial  to learn how easy this is to setup.  Webhooks \nUsing the URL printed out when the  bst proxy  starts up, you can access your local service from across the web.   Take the URL that is printed out for you and see that you can hit it from anywhere on the web.  This is extremely useful for debugging Webhook-based services (like Facebook Messenger Platform, Alexa Skills, Slack Webhooks, Microsoft Bot framework, etc.) that require having a public-facing server.  Deployment \nFor Node.js Lambdas, we have a one-step deploy tool. You can  read about it here .  Alexa Skills \nWe have built this originally to make it super-easy to develop Alexa skills. We offer an  array of commands and APIs  that aim to make developing for Alexa faster and more bullet-proof.", 
            "title": "More Neat Stuff"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_debugger/", 
            "text": "This tutorial shows you how to get setup using Webstorm to debug your AWS Lambdas locally. More tutorials to come for other IDEs (though the basic approach is the same :-)).\n\n\nUsing this, you can run AWS Lambdas directly on your machine, and step through the actual Lambda code to debug it with real requests and responses.\n\n\nPrerequisites\n\n\n\n\nBespoken Tools (bst)\n\n\nInstall bespoken-tools as part of your project\n\n\nnpm install bespoken-tools --save-dev\n\n\n\n\n\n\nWebStorm\n\n\nDownload site\n\n\n\n\n\n\n\n\nThese examples are based on a simple sample project. If  you want to use the same, just clone this repo:\n\n\ngit clone https://github.com/bespoken/lambda_samples\n\n\n\n\nOf course, feel free to use your own existing Lambda project.\n\n\nConfiguring WebStorm\n\n\nFrom your AWS Lambda project in WebStorm, right-click on the Lambda JS file and select 'Create \nLambdaFile.js\n':\n\n\n\n\nFill out the configuration:\n\n\n\n\nMake sure the JavaScript File is set to:\n\n\nnode_modules/bespoken-tools/bin/bst-proxy.js\n\n\nSet the application parameters with 'lambda' followed by the filename of the Lambda entry-point:\n\n\nlambda \nLambdaFile.js\n --verbose\n\n\n(The --verbose parameter prints out helpful information to the console).\n\n\nSelect 'OK' to save the configuration.\n\n\nTo start debugging, click on the little bug icon next to the configuration at the top:\n\n\n\nSeeing It In Action\n\n\nWe will use a simple curl to send a request to the service:\n\n\ncurl -H \nContent-Type: application/json\n \\\n  -X POST \\\n  -d '{\nkey1\n:\nvalue1\n,\nkey2\n:\nvalue2\n, \nkey3\n: \nvalue3\n}' \\\n  http://localhost:10000\n\n\n\n\nWith a breakpoint added at Line 7, here is what you will see:\n\n\n\n\nLots of great information, right? WebStorm offers a host of capabilities via their debugger - \nyou can learn more here\n.\n\n\nWe hope this helps accelerate how you develop and debug with Lambdas.", 
            "title": "Debugging Lambdas Locally"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_debugger/#prerequisites", 
            "text": "Bespoken Tools (bst)  Install bespoken-tools as part of your project  npm install bespoken-tools --save-dev    WebStorm  Download site     These examples are based on a simple sample project. If  you want to use the same, just clone this repo:  git clone https://github.com/bespoken/lambda_samples  Of course, feel free to use your own existing Lambda project.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_debugger/#configuring-webstorm", 
            "text": "From your AWS Lambda project in WebStorm, right-click on the Lambda JS file and select 'Create  LambdaFile.js ':   Fill out the configuration:   Make sure the JavaScript File is set to:  node_modules/bespoken-tools/bin/bst-proxy.js  Set the application parameters with 'lambda' followed by the filename of the Lambda entry-point:  lambda  LambdaFile.js  --verbose  (The --verbose parameter prints out helpful information to the console).  Select 'OK' to save the configuration.  To start debugging, click on the little bug icon next to the configuration at the top:", 
            "title": "Configuring WebStorm"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_debugger/#seeing-it-in-action", 
            "text": "We will use a simple curl to send a request to the service:  curl -H  Content-Type: application/json  \\\n  -X POST \\\n  -d '{ key1 : value1 , key2 : value2 ,  key3 :  value3 }' \\\n  http://localhost:10000  With a breakpoint added at Line 7, here is what you will see:   Lots of great information, right? WebStorm offers a host of capabilities via their debugger -  you can learn more here .  We hope this helps accelerate how you develop and debug with Lambdas.", 
            "title": "Seeing It In Action"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/", 
            "text": "This tutorial shows you how to use the Bespoken tools to test and deploy your Node.js AWS Lambdas.\nIt will also show you how to install a Lambda that will access resources inside your private VPC.\n\n\nUse case\n\n\nWe would like to create an Alexa skill that tells random quotes from famous people.\nI'm sure nobody thought about this before...\n\n\nWe will try two versions. A simple skill that chooses a random quote from a static array. \nTo make it more interesting, later we will pull the quotes from a local AWS resource, a REST service with Mongo \nwe installed on an EC2 instance. \n\n\nPrerequisites\n\n\nInstall bst\n\n\n$ npm install bespoken-tools -g\n\n\n\n\nFull Installation Instructions\n\n\nAWS Privileges\n\n\nMake sure you have the privileges to manage Lambdas on AWS. Ok, it's obvious, I know...\n\n\nYou will need AWS access keys - the secret access key and the access key id.\nIf you have the AWS command line tools installed, you probably already have these keys in the ~/.aws/credentials file.\nInstalling the AWS CLI is not required, but it is very useful. \n\n\nAlternatively you can set these two shell environment variables with the respective values.\n\n\n$ export AWS_SECRET_ACCESS_KEY=...\n$ export AWS_ACCESS_KEY_ID=...\n\n\n\n\nNote\n\nIt's not the scope of this tutorial to pontificate about security, but please don't use your root AWS credentials!\n\n\nZip\n\n\nYou may need to install \nzip\n depending on your operating system. It's needed for packaging the Lambdas.\n\n\nCreating The Lambda Skill\n\n\nThe sources for this demo are \nhere\n. Feel free to clone the repo.\n\n\nThere are two folders. The \nquote-server-mongo\n is a simple express REST service with Mongo. \nWe will use that for the backend later. If you want to try the demo end-to-end, then copy the code to your\nEC2 instance. Simple instructions to run the service are in the README.md file.\n\n\nThe actual skill Lambda is in the \nquote-skill\n folder. Navigate to the folder and run\n\n\nnpm install\n\n\n\n\nImportant\n\nYour project have to follow the node.js conventions. That is you need a package.json in top the level folder.\n\n\nNow let's take a peek into index.js. That is our Lambda. Very simple skill, \nwired to fetch the quotes from a static array by default.\n\n\nWe used Matt Kruse's \nexcellent SDK\n.\n\n\nvar Alexa = require('alexa-app');\nvar skill = new Alexa.app('randomquote');\n\nvar dataProvider = require('./dataProviderStatic');\n\nskill.launch(function(request, response) {\n    response\n        .say(\nspeak\nGreetings! You can say random quote to hear a quote.\n/speak\n)\n        .reprompt(\nspeak\nNo kidding! Just say it!\n/speak\n)\n        .shouldEndSession(false);\n});\n\nskill.intent('RandomQuote',\n    {\n        \nutterances\n: [\n            \nTell me a Quote\n,\n            \nRandom Quote\n\n        ]\n    },\n    function(request, response) {\n        dataProvider.findItem(function(quote) {\n            if (quote) {\n                response\n                    .say(\nspeak\n \n + quote.author + \n said once \n + quote.text + \n \n/speak\n)\n                    .reprompt(\nspeak\nYou can say random quote to hear a quote.\n/speak\n)\n                    .shouldEndSession(false).send();\n            } else {\n                response\n                    .say(\nspeak\nWe have a database connection error. Sorry!\n/speak\n)\n                    .shouldEndSession(true).send();\n            }\n        });\n\n        // Async (!)\n        return false;\n    }\n);\n\n// Expose the handler\nexports.handler = skill.lambda();\n\n\n\n\nKick the tires with BST\n\n\nStart the BST proxy to expose the Lambda for the BST tools.\n\n\n$ cd quote-skill/\n$ bst proxy lambda index.js \nBST: v0.9.13  Node: v6.3.0\n\nYour URL for Alexa Skill configuration:\nhttps://proxy.bespoken.tools?node-id=bdd4aad4-2339-4e85-aa33-2bbcd2e03168\n\nINFO  2016-11-04T01:32:01.139Z LambdaServer started on port: 10000\nINFO  2016-11-04T01:32:01.250Z Connected - proxy.bespoken.tools:5000\n\n\n\n\nNow let's \"tell\" something to our skill. From another shell session run the BST \nspeak\n command!\n\n\n\"random qoute\" is one of the sample utterances. The command prints out the matching request (intent) and the response. \nNo need to go to the Amazon Alexa UI to do basic tests. \n\n\nYou can read more about the \nbst speak\n command \nhere\n\n\n$ bst speak random quote\nBST: v0.9.13  Node: v6.3.0\n\nIntent: RandomQuote Session: null\nINFO  2016-11-04T01:35:10.096Z CALLING: IntentRequest\nSpoke: random quote\n\nRequest:\n{\n    \nrequest\n: {\n        \ntype\n: \nIntentRequest\n,\n        \nlocale\n: \nen-US\n,\n        \nrequestId\n: \namzn1.echo-api.request.5c3e96b6-0de7-4b7c-9537-95bc7b17b03a\n,\n        \ntimestamp\n: \n2016-11-04T01:35:10Z\n,\n        \nintent\n: {\n            \nname\n: \nRandomQuote\n\n        }\n    },\n    \ncontext\n: {\n        \nSystem\n: {\n            \napplication\n: {\n                \napplicationId\n: \namzn1.echo-sdk-ams.app.1a5e4446-234c-4dcf-84fb-05198a631305\n\n            },\n            \ndevice\n: {\n                \nsupportedInterfaces\n: {\n                    \nAudioPlayer\n: {}\n                }\n            },\n            \nuser\n: {\n                \nuserId\n: \namzn1.ask.account.4f906b29-d5e5-4c42-adfc-d6c8d2fc6a9c\n\n            }\n        },\n        \nAudioPlayer\n: {\n            \nplayerActivity\n: \nIDLE\n\n        }\n    },\n    \nversion\n: \n1.0\n,\n    \nsession\n: {\n        \nsessionId\n: \nSessionID.5e17be6c-be98-48e0-aa35-038e94d3e13e\n,\n        \napplication\n: {\n            \napplicationId\n: \namzn1.echo-sdk-ams.app.1a5e4446-234c-4dcf-84fb-05198a631305\n\n        },\n        \nuser\n: {\n            \nuserId\n: \namzn1.ask.account.4f906b29-d5e5-4c42-adfc-d6c8d2fc6a9c\n\n        },\n        \nnew\n: true,\n        \nattributes\n: {}\n    }\n}\n\nResponse:\n{\n    \nversion\n: \n1.0\n,\n    \nsessionAttributes\n: {},\n    \nresponse\n: {\n        \nshouldEndSession\n: false,\n        \noutputSpeech\n: {\n            \ntype\n: \nSSML\n,\n            \nssml\n: \nspeak\nYogi Berra said once I never said half the things I said.\n/speak\n\n        },\n        \nreprompt\n: {\n            \noutputSpeech\n: {\n                \ntype\n: \nSSML\n,\n                \nssml\n: \nspeak\nYou can say random quote to hear a quote.\n/speak\n\n            }\n        }\n    }\n}\n\n\n\n\nIf something isn't right, you can debug the skill locally with BST. \nYou can step through your code using your favorite IDE. \nFollow this tutorial\n \nto learn how easy that is.\n\n\nInstall the Lambda\n\n\nEverything meant to be easy with BST. The installation is no exception. BST has a \"one line deployer\" feature.\n\n\nAll you have to do is running the following command. The last parameter is the path to the Lambda project.\nIt could be relative or absolute path.\n\n\n$ bst deploy lambda . \nINFO  2016-11-04T00:22:31.654Z No configuration. Creating one: /Users/opendog/.bst/config\nBST: v0.9.13  Node: v6.3.0\n\nWe named your lambda function quote-skill (same as the project folder)\nWe created a AWS role for your lambda and called it lambda-bst-execution. You are welcome!\nNote that this lambda execution role is very basic. You may have to customize it on the AWS console!\nWaiting for AWS to propagate the changes\nWaiting for AWS to propagate the changes\nZip file(s) done uploading.\nEnter this ARN(s) on the Configuration tab of your skill:\n\n    arn:aws:lambda:us-east-1:6376:function:quote-skill\n\n$ \n\n\n\n\nWhat happened? \n\n\n\n\nThe tool created (or updated if it existed) the BST configuration file in \n~/.bst\n where you can tweak the install parameters later\n\n\nCreated a role called \nlambda-bst-execution\n for your Lambda with an associated basic policy\n\n\nPackaged and uploaded your Lambda to AWS\n\n\n\n\nYou can ask BST to give a specific name to the function with the \n--lambdaName\n option.\nOtherwise the tool uses the folder name. \n\n\nAt the end BST handed you over the \narn\n to setup your Lambda skill on the Alexa setup UI.\n\n\nThat was it! You can now modify your code and call the \nbst deploy\n command again the same way. BST will update your Lambda.\n\n\nCustomization\n\n\nConfig file\n\n\nLet's take a look at the BST config file I mentioned before. It looks like this:\n\n\n{\n    \nnodeID\n: \nbdd4aad4-2339-4e85-aa33-2bbcd2e03168\n,\n    \nlambdaDeploy\n: {\n        \nruntime\n: \nnodejs4.3\n,\n        \nrole\n: \nlambda-bst-execution\n,\n        \nhandler\n: \nindex.handler\n,\n        \ndescription\n: \nMy BST lambda skill\n,\n        \ntimeout\n: 3,\n        \nmemorySize\n: 128,\n        \nvpcSubnets\n: \n,\n        \nvpcSecurityGroups\n: \n,\n        \nexcludeGlobs\n: \nevent.json\n\n    }\n}\n\n\n\n\nYou can tailor these parameters as you wish. They are similar to the AWS API parameters except the \nexcludeGlobs\n.\nThe \nexcludeGlobs\n is a comma delimited list of files and folders that you can exclude from the packaged lambda.\nThings you don't need at runtime.\n\n\nThe \nnodeID\n is used by the BST proxy to expose your Lambda or local http endpoint on the public internet.\n\n\nLambda role\n\n\nThe \nlambda-bst-execution\n role only has the basics. Access rights to logging, S3 (put, get) and Dynamo (persistent storage).\nIf you need more access rights, you will have to modify it on the AWS console.\n\n\nThis is the default policy:\n\n\n{\n    \nVersion\n: \n2012-10-17\n,\n    \nStatement\n: [\n        {\n            \nSid\n: \n,\n            \nAction\n: [\n                \ndynamodb:DeleteItem\n,\n                \ndynamodb:GetItem\n,\n                \ndynamodb:PutItem\n,\n                \ndynamodb:Query\n,\n                \ndynamodb:Scan\n,\n                \ndynamodb:UpdateItem\n\n            ],\n            \nEffect\n: \nAllow\n,\n            \nResource\n: \n*\n\n        },\n        {\n            \nEffect\n: \nAllow\n,\n            \nAction\n: [\n                \nlogs:*\n\n            ],\n            \nResource\n: \narn:aws:logs:*:*:*\n\n        },\n        {\n            \nEffect\n: \nAllow\n,\n            \nAction\n: [\n                \ns3:GetObject\n\n            ],\n            \nResource\n: \narn:aws:s3:::'$source_bucket'/*\n\n        },\n        {\n            \nEffect\n: \nAllow\n,\n            \nAction\n: [\n                \ns3:PutObject\n\n            ],\n            \nResource\n: \narn:aws:s3:::'$target_bucket'/*\n\n        }\n    ]\n}\n\n\n\n\nLocal resources within your VPC\n\n\nLambdas should be written in a stateless manner. This means we cannot maintain a connection pool to a database.\nOne solution is a REST microservice that looks up the data for you.\n\n\nThis is what our little Express application does. I have already installed it on an EC2 instance (no kidding)\nand it's eagerly listening on port 3000.\n\n\nModify the skill code\n\n\nTo pull the quotes from our REST service just change the data provider import line in the skill (\nindex.js\n)\nfrom \n\n\nvar dataProvider = require('./dataProviderStatic');\n\n\n\n\nto\n\n\nvar dataProvider = require('./dataProvider');\n\n\n\n\nAlso change the REST server IP address and port in the data provider (\ndataProvider.js\n).\n\n\nAdd VPC subnet and security group to you Lambda function\n\n\nBefore our Lambda can access private VPC resources we need a few things.\n\n\nWe need to update the policy on the Lambda execution role\n\n\nBy default BST does not grant access to ENI (elastic network interface) functions. \nFuture will tell if this is a bug or a feature, but for now you have to add it manually.\nThe reason is security. I'm open for a debate.\n\n\nImportant\n\nYou need to add ENI access to the policy on the \nLambda execution role\n, \nnot on the role of the entity that installs the Lambda!\n\n\nGo to the \nIAM \n Roles\n menu on the AWS console and select \nlambda-bst-execution\n,\nand edit the policy. It's called \nlambda-bst-execution-access\n by default. \n\n\nAdd this to the JSON array and save:\n\n\n        {\n            \nEffect\n: \nAllow\n,\n            \nResource\n: \n*\n,\n            \nAction\n: [\n                \nec2:CreateNetworkInterface\n,\n                \nec2:DeleteNetworkInterface\n,\n                \nec2:DescribeNetworkInterfaces\n\n            ]\n        }\n\n\n\n\nWe also need the subnet id of our server's EC2 instance and a security group id\n\n\nImportant\n\nWe need the ids not the names!\n\n\nThe vpc subnet id is the \nSubnet ID\n field on the EC2 instance detail page. It starts with \nsubnet-\n\n\nTo find the security group id, click on the security group on the EC2 instance detail page (on the right). \nIf you used the AWS wizard to spin up the instance, the name will be something like \nlaunch-wizard-1\n.\nYou can use that group. The id starts with \nsg-\n. \n\n\nIt probably already has the ssh (22) port open. Let's add our server port, the 3000.\n\n\n\n\nAdd the ids to the BST config file in \n~/.bst/config\n like this:\n\n\n{\n    \nnodeID\n: \nfdd376c8-3d3e-74-41924af\n,\n    \nlambdaDeploy\n: {\n        \nruntime\n: \nnodejs4.3\n,\n        \nrole\n: \nlambda-bst-execution\n,\n        \nhandler\n: \nindex.handler\n,\n        \ndescription\n: \nMy BST lambda skill\n,\n        \ntimeout\n: 3,\n        \nmemorySize\n: 128,\n        \nvpcSubnets\n: \nsubnet-e6dcb\n,\n        \nvpcSecurityGroups\n: \nsg-c66b7\n,\n        \nexcludeGlobs\n: \nevent.json\n\n    }\n}\n\n\n\n\nUpdate the Lambda\n\n\nThe Lambda function code update is easy with BST. The same command will update the Lambda. \nFrom the Lambda project folder run this:\n\n\n$ bst deploy lambda . \nBST: v0.9.13  Node: v6.3.0\n\nWe named your lambda function quote-skill (same as the project folder)\nRe-using existing BST lambda role.\nZip file(s) done uploading.\nEnter this ARN(s) on the Configuration tab of your skill:\n\n    arn:aws:lambda:us-east-1:876:function:quote-skill\n\n$ \n\n\n\n\nI hope this was helpful. If you have any questions or problems with this tutorial \nyou can email me at bela@opendog.org.", 
            "title": "Deploying Lambdas"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/#use-case", 
            "text": "We would like to create an Alexa skill that tells random quotes from famous people.\nI'm sure nobody thought about this before...  We will try two versions. A simple skill that chooses a random quote from a static array. \nTo make it more interesting, later we will pull the quotes from a local AWS resource, a REST service with Mongo \nwe installed on an EC2 instance.", 
            "title": "Use case"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/#prerequisites", 
            "text": "", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/#install-bst", 
            "text": "$ npm install bespoken-tools -g  Full Installation Instructions", 
            "title": "Install bst"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/#aws-privileges", 
            "text": "Make sure you have the privileges to manage Lambdas on AWS. Ok, it's obvious, I know...  You will need AWS access keys - the secret access key and the access key id.\nIf you have the AWS command line tools installed, you probably already have these keys in the ~/.aws/credentials file.\nInstalling the AWS CLI is not required, but it is very useful.   Alternatively you can set these two shell environment variables with the respective values.  $ export AWS_SECRET_ACCESS_KEY=...\n$ export AWS_ACCESS_KEY_ID=...  Note \nIt's not the scope of this tutorial to pontificate about security, but please don't use your root AWS credentials!", 
            "title": "AWS Privileges"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/#zip", 
            "text": "You may need to install  zip  depending on your operating system. It's needed for packaging the Lambdas.", 
            "title": "Zip"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/#creating-the-lambda-skill", 
            "text": "The sources for this demo are  here . Feel free to clone the repo.  There are two folders. The  quote-server-mongo  is a simple express REST service with Mongo. \nWe will use that for the backend later. If you want to try the demo end-to-end, then copy the code to your\nEC2 instance. Simple instructions to run the service are in the README.md file.  The actual skill Lambda is in the  quote-skill  folder. Navigate to the folder and run  npm install  Important \nYour project have to follow the node.js conventions. That is you need a package.json in top the level folder.  Now let's take a peek into index.js. That is our Lambda. Very simple skill, \nwired to fetch the quotes from a static array by default.  We used Matt Kruse's  excellent SDK .  var Alexa = require('alexa-app');\nvar skill = new Alexa.app('randomquote');\n\nvar dataProvider = require('./dataProviderStatic');\n\nskill.launch(function(request, response) {\n    response\n        .say( speak Greetings! You can say random quote to hear a quote. /speak )\n        .reprompt( speak No kidding! Just say it! /speak )\n        .shouldEndSession(false);\n});\n\nskill.intent('RandomQuote',\n    {\n         utterances : [\n             Tell me a Quote ,\n             Random Quote \n        ]\n    },\n    function(request, response) {\n        dataProvider.findItem(function(quote) {\n            if (quote) {\n                response\n                    .say( speak    + quote.author +   said once   + quote.text +    /speak )\n                    .reprompt( speak You can say random quote to hear a quote. /speak )\n                    .shouldEndSession(false).send();\n            } else {\n                response\n                    .say( speak We have a database connection error. Sorry! /speak )\n                    .shouldEndSession(true).send();\n            }\n        });\n\n        // Async (!)\n        return false;\n    }\n);\n\n// Expose the handler\nexports.handler = skill.lambda();", 
            "title": "Creating The Lambda Skill"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/#kick-the-tires-with-bst", 
            "text": "Start the BST proxy to expose the Lambda for the BST tools.  $ cd quote-skill/\n$ bst proxy lambda index.js \nBST: v0.9.13  Node: v6.3.0\n\nYour URL for Alexa Skill configuration:\nhttps://proxy.bespoken.tools?node-id=bdd4aad4-2339-4e85-aa33-2bbcd2e03168\n\nINFO  2016-11-04T01:32:01.139Z LambdaServer started on port: 10000\nINFO  2016-11-04T01:32:01.250Z Connected - proxy.bespoken.tools:5000  Now let's \"tell\" something to our skill. From another shell session run the BST  speak  command!  \"random qoute\" is one of the sample utterances. The command prints out the matching request (intent) and the response. \nNo need to go to the Amazon Alexa UI to do basic tests.   You can read more about the  bst speak  command  here  $ bst speak random quote\nBST: v0.9.13  Node: v6.3.0\n\nIntent: RandomQuote Session: null\nINFO  2016-11-04T01:35:10.096Z CALLING: IntentRequest\nSpoke: random quote\n\nRequest:\n{\n     request : {\n         type :  IntentRequest ,\n         locale :  en-US ,\n         requestId :  amzn1.echo-api.request.5c3e96b6-0de7-4b7c-9537-95bc7b17b03a ,\n         timestamp :  2016-11-04T01:35:10Z ,\n         intent : {\n             name :  RandomQuote \n        }\n    },\n     context : {\n         System : {\n             application : {\n                 applicationId :  amzn1.echo-sdk-ams.app.1a5e4446-234c-4dcf-84fb-05198a631305 \n            },\n             device : {\n                 supportedInterfaces : {\n                     AudioPlayer : {}\n                }\n            },\n             user : {\n                 userId :  amzn1.ask.account.4f906b29-d5e5-4c42-adfc-d6c8d2fc6a9c \n            }\n        },\n         AudioPlayer : {\n             playerActivity :  IDLE \n        }\n    },\n     version :  1.0 ,\n     session : {\n         sessionId :  SessionID.5e17be6c-be98-48e0-aa35-038e94d3e13e ,\n         application : {\n             applicationId :  amzn1.echo-sdk-ams.app.1a5e4446-234c-4dcf-84fb-05198a631305 \n        },\n         user : {\n             userId :  amzn1.ask.account.4f906b29-d5e5-4c42-adfc-d6c8d2fc6a9c \n        },\n         new : true,\n         attributes : {}\n    }\n}\n\nResponse:\n{\n     version :  1.0 ,\n     sessionAttributes : {},\n     response : {\n         shouldEndSession : false,\n         outputSpeech : {\n             type :  SSML ,\n             ssml :  speak Yogi Berra said once I never said half the things I said. /speak \n        },\n         reprompt : {\n             outputSpeech : {\n                 type :  SSML ,\n                 ssml :  speak You can say random quote to hear a quote. /speak \n            }\n        }\n    }\n}  If something isn't right, you can debug the skill locally with BST. \nYou can step through your code using your favorite IDE.  Follow this tutorial  \nto learn how easy that is.", 
            "title": "Kick the tires with BST"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/#install-the-lambda", 
            "text": "Everything meant to be easy with BST. The installation is no exception. BST has a \"one line deployer\" feature.  All you have to do is running the following command. The last parameter is the path to the Lambda project.\nIt could be relative or absolute path.  $ bst deploy lambda . \nINFO  2016-11-04T00:22:31.654Z No configuration. Creating one: /Users/opendog/.bst/config\nBST: v0.9.13  Node: v6.3.0\n\nWe named your lambda function quote-skill (same as the project folder)\nWe created a AWS role for your lambda and called it lambda-bst-execution. You are welcome!\nNote that this lambda execution role is very basic. You may have to customize it on the AWS console!\nWaiting for AWS to propagate the changes\nWaiting for AWS to propagate the changes\nZip file(s) done uploading.\nEnter this ARN(s) on the Configuration tab of your skill:\n\n    arn:aws:lambda:us-east-1:6376:function:quote-skill\n\n$   What happened?    The tool created (or updated if it existed) the BST configuration file in  ~/.bst  where you can tweak the install parameters later  Created a role called  lambda-bst-execution  for your Lambda with an associated basic policy  Packaged and uploaded your Lambda to AWS   You can ask BST to give a specific name to the function with the  --lambdaName  option.\nOtherwise the tool uses the folder name.   At the end BST handed you over the  arn  to setup your Lambda skill on the Alexa setup UI.  That was it! You can now modify your code and call the  bst deploy  command again the same way. BST will update your Lambda.", 
            "title": "Install the Lambda"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/#customization", 
            "text": "", 
            "title": "Customization"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/#config-file", 
            "text": "Let's take a look at the BST config file I mentioned before. It looks like this:  {\n     nodeID :  bdd4aad4-2339-4e85-aa33-2bbcd2e03168 ,\n     lambdaDeploy : {\n         runtime :  nodejs4.3 ,\n         role :  lambda-bst-execution ,\n         handler :  index.handler ,\n         description :  My BST lambda skill ,\n         timeout : 3,\n         memorySize : 128,\n         vpcSubnets :  ,\n         vpcSecurityGroups :  ,\n         excludeGlobs :  event.json \n    }\n}  You can tailor these parameters as you wish. They are similar to the AWS API parameters except the  excludeGlobs .\nThe  excludeGlobs  is a comma delimited list of files and folders that you can exclude from the packaged lambda.\nThings you don't need at runtime.  The  nodeID  is used by the BST proxy to expose your Lambda or local http endpoint on the public internet.", 
            "title": "Config file"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/#lambda-role", 
            "text": "The  lambda-bst-execution  role only has the basics. Access rights to logging, S3 (put, get) and Dynamo (persistent storage).\nIf you need more access rights, you will have to modify it on the AWS console.  This is the default policy:  {\n     Version :  2012-10-17 ,\n     Statement : [\n        {\n             Sid :  ,\n             Action : [\n                 dynamodb:DeleteItem ,\n                 dynamodb:GetItem ,\n                 dynamodb:PutItem ,\n                 dynamodb:Query ,\n                 dynamodb:Scan ,\n                 dynamodb:UpdateItem \n            ],\n             Effect :  Allow ,\n             Resource :  * \n        },\n        {\n             Effect :  Allow ,\n             Action : [\n                 logs:* \n            ],\n             Resource :  arn:aws:logs:*:*:* \n        },\n        {\n             Effect :  Allow ,\n             Action : [\n                 s3:GetObject \n            ],\n             Resource :  arn:aws:s3:::'$source_bucket'/* \n        },\n        {\n             Effect :  Allow ,\n             Action : [\n                 s3:PutObject \n            ],\n             Resource :  arn:aws:s3:::'$target_bucket'/* \n        }\n    ]\n}", 
            "title": "Lambda role"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/#local-resources-within-your-vpc", 
            "text": "Lambdas should be written in a stateless manner. This means we cannot maintain a connection pool to a database.\nOne solution is a REST microservice that looks up the data for you.  This is what our little Express application does. I have already installed it on an EC2 instance (no kidding)\nand it's eagerly listening on port 3000.", 
            "title": "Local resources within your VPC"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/#modify-the-skill-code", 
            "text": "To pull the quotes from our REST service just change the data provider import line in the skill ( index.js )\nfrom   var dataProvider = require('./dataProviderStatic');  to  var dataProvider = require('./dataProvider');  Also change the REST server IP address and port in the data provider ( dataProvider.js ).", 
            "title": "Modify the skill code"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/#add-vpc-subnet-and-security-group-to-you-lambda-function", 
            "text": "Before our Lambda can access private VPC resources we need a few things.", 
            "title": "Add VPC subnet and security group to you Lambda function"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/#we-need-to-update-the-policy-on-the-lambda-execution-role", 
            "text": "By default BST does not grant access to ENI (elastic network interface) functions. \nFuture will tell if this is a bug or a feature, but for now you have to add it manually.\nThe reason is security. I'm open for a debate.  Important \nYou need to add ENI access to the policy on the  Lambda execution role , \nnot on the role of the entity that installs the Lambda!  Go to the  IAM   Roles  menu on the AWS console and select  lambda-bst-execution ,\nand edit the policy. It's called  lambda-bst-execution-access  by default.   Add this to the JSON array and save:          {\n             Effect :  Allow ,\n             Resource :  * ,\n             Action : [\n                 ec2:CreateNetworkInterface ,\n                 ec2:DeleteNetworkInterface ,\n                 ec2:DescribeNetworkInterfaces \n            ]\n        }", 
            "title": "We need to update the policy on the Lambda execution role"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/#we-also-need-the-subnet-id-of-our-servers-ec2-instance-and-a-security-group-id", 
            "text": "Important \nWe need the ids not the names!  The vpc subnet id is the  Subnet ID  field on the EC2 instance detail page. It starts with  subnet-  To find the security group id, click on the security group on the EC2 instance detail page (on the right). \nIf you used the AWS wizard to spin up the instance, the name will be something like  launch-wizard-1 .\nYou can use that group. The id starts with  sg- .   It probably already has the ssh (22) port open. Let's add our server port, the 3000.   Add the ids to the BST config file in  ~/.bst/config  like this:  {\n     nodeID :  fdd376c8-3d3e-74-41924af ,\n     lambdaDeploy : {\n         runtime :  nodejs4.3 ,\n         role :  lambda-bst-execution ,\n         handler :  index.handler ,\n         description :  My BST lambda skill ,\n         timeout : 3,\n         memorySize : 128,\n         vpcSubnets :  subnet-e6dcb ,\n         vpcSecurityGroups :  sg-c66b7 ,\n         excludeGlobs :  event.json \n    }\n}", 
            "title": "We also need the subnet id of our server's EC2 instance and a security group id"
        }, 
        {
            "location": "/tutorials/tutorial_lambda_deploy/#update-the-lambda", 
            "text": "The Lambda function code update is easy with BST. The same command will update the Lambda. \nFrom the Lambda project folder run this:  $ bst deploy lambda . \nBST: v0.9.13  Node: v6.3.0\n\nWe named your lambda function quote-skill (same as the project folder)\nRe-using existing BST lambda role.\nZip file(s) done uploading.\nEnter this ARN(s) on the Configuration tab of your skill:\n\n    arn:aws:lambda:us-east-1:876:function:quote-skill\n\n$   I hope this was helpful. If you have any questions or problems with this tutorial \nyou can email me at bela@opendog.org.", 
            "title": "Update the Lambda"
        }
    ]
}